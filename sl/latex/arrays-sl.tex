\translatedby{Jan Bratina}{sl}
\chapter{Implementacija seznama s poljem}
\chaplabel{arrays}

V tem poglavju si bomo pogledali izvedbe vmesnikov #Seznama# in #Vrste#,
kjer je osnoven podatek hranjen v polju, imenovanem 
\emph{podporno polje}.
\index{podporno polje}%
V spodnji tabeli imamo časovne zahtevnosti operacij za podatkovne strukture predstavljene v tem poglavju:
\newlength{\tabsep}
\setlength{\tabsep}{\itemsep}
\addtolength{\tabsep}{\parsep}
\addtolength{\tabsep}{-2pt}
\begin{center}
\vspace{\tabsep}
\begin{tabular}{|l|l|l|} \hline
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# \\ \hline
#ArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\
#ArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#DualArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#RootishArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\ \hline
\end{tabular}
\vspace{\tabsep}
\end{center}
V podatkovnih strukturah, kjer podatke shranjujemo v enojno polje imajo veliko prednosti, 
a tudi omejitev: 
\index{arrays}%
\begin{itemize}
  \item V polju imamo vedno konstantni čas za dostop do kateregakoli podatka. 
  Zato se nam operaciji #get(i)# in #set(i,x)# izvedejo v konstantnem času.

  \item Polja niso dinamična. Če želimo vstaviti ali pa izbrisati element v sredini polja moramo premakniti veliko elementov, da naredimo prostor za novo vstavljen element oz. da zapolnimo praznino po tem ko smo element izbrisali. Zato je časovna zahtevnost operacij #add(i,x)# in #remove(i)# odvisna od spremenljivk #n# in #i#.

  \item Polja ne moremo širiti ali krčiti. Ko imamo večje število elementov, kot je veliko naše podporno polje, moramo ustvariti novo, dovolj veliko polje, v katerega kopiramo podatke iz prejšnjega polja. Ta operacija je zelo draga. 
\end{itemize}
Tretja točka je zelo pomembna, saj časovne zahtevnosti iz zgornje tabele ne vključujejo spreminjanja velikosti polja. V nadaljevanju bomo videli, da širjenje in krčenje polja ne dodata veliko k \emph{povprečni} časovni zahtevnosti, če jih ustrezno upravljamo.  Če natančneje pogledamo, če začnemo s prazno podatkovno strukturo in izvedemo sekvenco operacij $m$ #add(i,x)# ali #remove(i)#
potem bo časovna zahtevnost širjenja in krčenja polja za $m$ operacij $O(m)$.  Čeprav so nekatere operacije dražje je povprečna časovna zahtevnost nad vsemi $m$ operacijami samo $O(1)$ za operacijo.

\cpponly{
V tem poglavju in v celotni knjigi je priročno uporabljati polja, ki imajo števec za velikost.  Navadna polja v C++ nimajo te funkcije, zato definiramo razred,  #array#, ki hrani dolžino polja.  Implementacija tega razreda je enostavna.
Implementiran je kot običajno C++ polje, #a#, in število, #length#:}
\cppimport{ods/array.a.length}
\cpponly{
Velikost polja #array# je določena od kreaciji:
}
\cppimport{ods/array.array(len)}
\cpponly{Elementi v polju so lahko indeksirani:}
\cppimport{ods/array.operator[]}
\cpponly{Na koncu, ko imamo eno polje dodeljeno drugemu, potrebujemo samo še premikanje kazalca, ki pa se izvede v konstantnem času:}
\cppimport{ods/array.operator=}

\section{#ArrayStack#: Implementacija sklada s poljem}
\seclabel{arraystack}

\index{ArrayStack@#ArrayStack#}%
Z operacijo #ArrayStack# implementiramo vmesnik za seznam z uporabo polja #a#, imenovanega the \emph{podporno polje}.  Element v seznamu na indexu #i# je hranjen v #a[i]#.  V večini primerov je velikost polja #a# večja, kot je potrebno, zato uporabimo število #n# kot števec števila elementov spravljenih v polju #a#.  Tako imamo elemente spravljene v
#a[0]#,\ldots,#a[n-1]# in v vseh primerih velja, $#a.length# \ge #n#$.

\codeimport{ods/ArrayStack.a.n.size()}

\subsection{Osnove}

Dostop in spreminjanje elementov v #ArrayStack# z uporabo operacij #get(i)#
in #set(i,x)# je zelo lahko. Po izvedbi potrebnih mejnih preverjanj polja vrnemo množico oz. #a[i]#.

\codeimport{ods/ArrayStack.get(i).set(i,x)}

Operaciji vstavljanja in brisanja elementov iz #ArrayStack#
sta predstavljeni v \figref{arraystack}.  Za implementacijo #add(i,x)#
operacije najprej preverimo, če je polje #a# polno. Če je, kličemo metodo #resize()# za povečanje velikosti polja #a#.  Kako je metoda #resize()#
implementirana si bomo pogledali kasneje, saj nas trenutno zanima samo to, da potem ko kličemo metodo #resize()#še vedno ohranjamo pogoj $#a.length#
> #n#$.  Sedaj lahko premaknemo elemente
$#a[i]#,\ldots,#a[n-1]#$ za ena v desno, da naredimo prostor za #x#,
množico #a[i]# spravimo v #x#, ain povečamo #n#, saj smo vstavili nov element.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraystack}
  \end{center}
  \caption[Dodajanje elementa v ArrayStack]{Sekvenca operacij #add(i,x)# in #remove(i)# v #ArrayStack#.  Puščice označujejo elemente, ki jih je potrebno kopirati. Operacije, po katerih moramo klicati metodo #resize()# so označene z zvezdico.}
  \figlabel{arraystack}
\end{figure}

\codeimport{ods/ArrayStack.add(i,x)}
Če zapostavimo časovno zahtevnost ob morebitnem klicanju metode #resize()#, potem je časovna zahtevnost operacije #add(i,x)# sorazmerna številu elementov, ki jih moramo premakniti, da naredimo prostor za novo vstavljen element #x#.  Zato je časovna zahtevnost operacije (zanemarimo časovno zahtevnost spreminjanja polja #a#) $O(#n#-#i#)$.

Implementacija operacije #remove(i)# je zelo podobna. Premaknemo elemente
$#a[i+1]#,\ldots,#a[n-1]#$ za ena v levo (prepišemo #a[i]#) in zmanjšamo vrednost #n#. Potem preverimo če števec #n# postaja občutno manjši kot #a.length# s preverjanjem $#a.length# \ge 3#n#$. Če je občutno manjši kličemo metodo #resize()# za zmanjšanje velikosti polja #a#.

\codeimport{ods/ArrayStack.remove(i)}
% TODO: Add shifting figure
Če zanemarimo časovno zahtevnost metode #resize()# je časovna zahtevnost operacije #remove(i)#
sorazmerna s številom elementov, ki jih moramo premakniti. To pomeni, da je časovna zahtevnost $O(#n#-#i#)$.

\translatedby{Luka Zorc}{sl}
\subsection{Večanje in krčenje}

Metoda #resize()# je dokaj enostavna; naredi novo polje #b# velikosti $2#n#$ in kopira #n#  elementov iz polja #a# v
prvih #n# mest polja #b#, in nato postavi #a# v #b#. Tako po klicu #resize()#, $#a.length# = 2#n#$.

\codeimport{ods/ArrayStack.resize()}

Analiza kompleksnosti operacije #resize()# je lahka. Metoda naredi polje #b# velikosti $2#n#$ in kopira #n# elementov iz #a#
v #b#.  To traja $O(#n#)$ časa.

Pri analizi časa delovanja iz prejšnjega poglavja ni bila ušteta cena klica #resize()# funkcije. V tem poglavju bomo analizirali to ceno z uporabo tehnike znane pod imenom \emph{amortizirana analiza}.  Ta način ne poskuša ugotoviti cene za spreminjanje velikosti med vsako #add(i,x)#
in #remove(i)# operacijo. Namesto tega, se posveti ceni vseh klicev
#resize()# med sekvenco $m$ klicov funkcije #add(i,x)# ali #remove(i)#.
Predvsem pokažemo:

\begin{lem}\lemlabel{arraystack-amortized}
  Če je ustvarjen prazen #ArrayList# in katerakoli sekvenca, ko je $m\ge 1$ kliče #add(i,x)# ali #remove(i)# potem je skupen porabljen čas za vse klice #resize()# enak $O(m)$.
\end{lem}

\begin{proof}
  Pokazali bomo, da vsakič ko je #resize()#  klican, je število klicev #add# ali #remove# od zadnjega klica #resize()# funkcije, vsaj
  $#n#/2-1$.  Torej, če $#n#_i$ označuje vrednost #n# med
  $i$tim klicem metode #resize()# in $r$ označuje število klicev funkcije
  #resize()#, potem je skupno število klicev #add(i,x)# ali
  #remove(i)# vsaj
  \[
     \sum_{i=1}^{r} (#n#_i/2-1) \le m  \enspace ,
  \]
  kar je enako kot
  \[
    \sum_{i=1}^{r} #n#_i \le 2m + 2r  \enspace .
  \]
  Na drugi strani, je skupno število časa uporabljenega med vsem #resize()# klici enako 
  \[
     \sum_{i=1}^{r} O(#n#_i) \le O(m+r) = O(m)  \enspace ,
  \]
  ker $r$ ni več kot $m$.  Vse kar nam ostane je pokazati da je število klicev #add(i,x)# ali #remove(i)# med the $(i-1)$tim
  in $i$timi klicem za #resize()# enako vsaj $#n#_i/2$.

  Upoštevati moramo dva primera. V prvem primeru, je bila metoda #resize()# klicana s strani funkcije #add(i,x)# ker je bilo polje #a# polno, t.j.,
  $#a.length# = #n#=#n#_i$.  Če pogledamo prejšnji klic funkcije #resize()#:
  po tem klicu, je bila velikost #a#-ja enaka #a.length#, vendar je bilo število elementov shranjenih v #a#-ju največ $#a.length#/2=#n#_i/2$.
  Zdaj pa je število elementov shranjenih v #a# enako $#n#_i=#a.length#$,
   torej se je moralo, od prejšnjega klica #resize()# zgoditi vsaj $#n#_i/2$ klicev #add(i,x)#.
  % TODO: Add figure
  
  Drugi primer se zgodi, ko je #resize()# klicana s strani funkcije
  #remove(i)#, ker je $#a.length# \ge 3#n#=3#n#_i$.  Enako kot prej je po prejšnjemu klicu #resize()# bilo število elementov shranjenih v #a# najmanj  $#a.length/2#-1$.\footnote{The ${}-1$ in this formula accounts for
  the special case that occurs when $#n#=0$ and $#a.length# = 1$.} Zdaj pa je v #a# shranjenih $#n#_i\le#a.length#/3$ elementov.  Zato je število #remove(i)# operacij od zadnjega  #resize()# klica vsaj
  \begin{align*}
      R & \ge #a.length#/2 - 1 - #a.length#/3 \\
        & = #a.length#/6 - 1 \\
        & = (#a.length#/3)/2 - 1 \\
        & \ge #n#_i/2 -1\enspace .
  \end{align*}
  V vsakem primeru je število klicev #add(i,x)# ali #remove(i)#, ki se zgodijo med $(i-1)$tim klicem za #resize()# in $i$tim klicem za
  #resize()# nako vsaj $#n#_i/2-1$, oliko kot je tudi potrebno za dokončanje dokaza.
\end{proof}

\subsection{Povzetek}

Naslednji izrek povzema kvaliteto izvedbe #ArrayStack#-a:

\begin{thm}\thmlabel{arraystack}
  #ArrayStack# implementira #List# vmesnik.  Če ignoriramo ceno klicev funkcije #resize()#, potem  #ArrayStack# podpira slednje operacije
  \begin{itemize}
    \item #get(i)# in #set(i,x)# v času $O(1)$ a eno operacijo; in
    \item #add(i,x)# in #remove(i)# v času $O(1+#n#-#i#)$ na operacijo.
  \end{itemize}
  Poleg tega, če začnemo z praznim #ArrayStack#-om in potem izvajamo katerokoli zaporedje od $m$ #add(i,x)# in #remove(i)# operacij to privede v skupno $O(m)$ časa uporabljenega med vsem klici funkcije #resize()#.
\end{thm}

#ArrayStack# je učinkovit način za implementiranje #Sklada#.
Funkcijo #push(x)# lahko implementiramo kot #add(n,x)# n funkcijo #pop()#
kot #remove(n-1)#, V tem primeru bodo te operacije potrebovale $O(1)$
amortiziranega časa.

\section{#FastArrayStack#: Optimiziran ArrayStack}
\seclabel{fastarraystack}

\index{FastArrayStack@#FastArrayStack#}%
Večina opravljenega dela #ArrayStack#-a vkjučuje premikanje (z
#add(i,x)# in #remove(i)#) in kopiranjem (z #resize()#) podatkov.
V izvedbah prikazanih zgoraj,  je bilo to narejeno s pomočjo #for# zanke. Izkaže se, da ima veliko programskih okolij posebne funkcije, ki so zelo učinkovite pri kopiranju in premikanju blokov podatkov. V programskem jeziku C, obstajajo #memcpy(d,s,n)# in #memmove(d,s,n)#
funkcije. V C++ jeziku je #std::copy(a0,a1,b)# algoritem.
V Javi je metoda #System.arraycopy(s,i,d,j,n)#.
\index{memcpy@#memcpy(d,s,n)#}%
\index{std::copy@#std::copy(a0,a1,b)#}%
\index{System.arraycopy@#System.arraycopy(s,i,d,j,n)#}%

\codeimport{ods/FastArrayStack.add(i,x).remove(i).resize()}

Te funkcije so ponavadi zelo optimizirane in lahko uporabljajo tudi posebne strojne ukaze, ki lahko kopirajo veliko hitreje, kot z uporabo zanke #for#. Čeprav s pomočjo teh funkcij ne moremo asimptotično zmanjšati izvajalnih časov, je ta optimizacija še vedno koristna.
V teh \lang\ izvedbah Jave, uporaba nativnega \javaonly{#System.arraycopy(s,i,d,j,n)#}\cpponly{#std::copy(a0,a1,b)#}
povzroči pohitritve za faktor med 2 in 3, odvisno od vrste izvajanih operacij. Naše izkušnje se lahko razlikujejo.


\translatedby{Jan Tomšič}{sl}
\section{#ArrayQueue#: Vrsta na osnovi polja}
\seclabel{arrayqueue}

\index{ArrayQueue@#ArrayQueue#}%
V tem poglavju bomo predstavili podatkovno strukturo #ArrayQueue#, ki implementira FIFO vrsto; elemente z vrste odstranjujemo (z uporabo operacije #remove()#) v istem vrstnem redu, kot so bili dodani (z uporabo operacije #add(x)#). 

Opazimo, da #ArrayStack# ni dobra izbira za izvedbo FIFO vrste in sicer zato, ker moramo izbrati en konec seznama, na katerega dodajamo elemente, nato pa elemente odstranjujemo z drugega konca.  Ena izmed operacij mora delovati na glavi seznama, kar vključuje klicanje #add(i,x)# ali #remove(i)#, kjer je vrednost $#i#=0$.  To nudi čas izvajanja sorazmeren #n#. 

Da bi dosegli učinkovito implementacijo vrste na osnovi seznama, najprej opazimo, da bi bil problem enostaven, če bi imeli neskočno polje #a#.  Lahko bi hranili indeks #j#, ki hrani naslednji element za odstranitev ter celo število #n#, ki šteje število elementov v vrsti.  Elementi vrste bi bili vedno shranjeni v \[ #a[j]#,#a[j+1]#,\ldots,#a[j+n-1]# \enspace . \]
Sprva bi bila #j# in #n# nastavljena na 0.  Na novo dodan element bi uvrstili v #a[j+n]# in povečali #n#.  Za odstranitev elementa bi ga odstranili iz #a[j]#, povečali #j# in zmanjšali #n#.

Težava te rešitve je potreba po neskočnem polju.  #ArrayQueue# to simulira z uporabo končnega polja in \emph{modularne aritmetike}.
\index{modularna aritmetika}%
To je vrsta aritmetike, ki jo uporabljamo pri napovedovanju časa.  Na primer 10:00 plus pet ur je 3:00.  Formalno pravimo, da je
\[
    10 + 5 = 15 \equiv 3 \pmod{12} \enspace .
\]
Zadnji del enačbe beremo kot ``15 je skladno s 3 po modulu 12.'' Operator $\bmod$ lahko obravnavamo tudi kot binarni operator, da je
\[
   15 \bmod 12 = 3 \enspace .
\]

V splošnem je, za celo število $a$ in pozitivno celo število $m$, $a \bmod m$ enolično celo število $r\in\{0,\ldots,m-1\}$ tako, da velja $a = r + km$ za poljubno celo število $k$.  Poenostavljeno vrednost $r$ predstavlja ostanek pri deljenju $a$ z $m$.  V večini programskih jezikov, vključno \javaonly{z Javo}\cpponly{s C++}, je operator $\bmod$ predstavljen z znakom #%#.\footnote{Temu včasih rečemo operator \emph{brain-dead}, ker nepravilno implementira matematični operator mod, ko je prvi argument negativno število.}

Modularna aritmetika je uporabna za simulacijo neskončnega polje, ker $#i#\bmod #a.length#$ vedno vrne vrednost na intervalu $0,\ldots,#a.length-1#$.  Z uporabo modularne aritmetike lahko elemente vrste shranimo na naslednja mesta v polju
\[ #a[j%a.length]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\enspace. \]
To obravnava polje #a# kot \emph{krožno polje}
\index{krožno polje}%
\index{polje!krožno}%
kjer polje indekse večje kot $#a.length#-1$ ``ovije naokrog'' na začetek polja.
% TODO: figure

Paziti moramo le še, da število elementov v #ArrayQueue# ne preseže velikosti #a#.

\codeimport{ods/ArrayQueue.a.j.n}

Zaporedje operacij #add(x)# in #remove()# nad #ArrayQueue# je prikazano na \figref{arrayqueue}.  Za izvedbo #add(x)# moramo najprej preveriti, če je #a# poln, in s klicem #resize()# velikost #a# povečati.  Nato #x# shranimo v #a[(j+n)%a.length]# in povečamo #n#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arrayqueue}
  \end{center}
  \caption[Dodajanje in odstranjevanje iz ArrayQueue]{Zaporedje operacij #add(x)# in #remove(i)# nad #ArrayQueue#.  Puščice označujejo kopiranje elementov.  Operacije, ki se zaključijo s klicem #resize()# so označene z zvezdico.}
  \figlabel{arrayqueue}
\end{figure}



\codeimport{ods/ArrayQueue.add(x)}

Za izvedbo #remove()# moramo najprej za kasnejšo rabo shraniti #a[j]#.  Nato zmanjšamo #n# in povečamo #j# (po modulu #a.length#) tako, da nastavimo $#j#=(#j#+1)\bmod #a.length#$.  Na koncu vrnemo shranjeno vrednost #a[j]#.  Po potrebi lahko zmanjšamo velikost #a# s klicem #resize()#.

\codeimport{ods/ArrayQueue.remove()}

Operacija #resize()# je zelo podobna operaciji #resize()# pri #ArrayStack#.  Dodeli novo polje #b# velikosti $2#n#$ in prepiše
\[
   #a[j]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\]
na
\[
   #b[0]#,#b[1]#,\ldots,#b[n-1]#
\]
in nastavi $#j#=0$.

\codeimport{ods/ArrayQueue.resize()}

\subsection{Povzetek}

Naslednji izrek povzame učinkovitost podatkovne strukture #ArrayQueue#:

\begin{thm}
#ArrayQueue# implementira vmesnik (FIFO) #Vrste#.  Če izvzamemo ceno klica #resize()#, omogoča #ArrayQueue# izvajanje operacij #add(x)# in #remove()# v času $O(1)$ na operacijo.  Poleg tega, začenši s prazno vrsto #ArrayQueue#, vsako zaporedje $m$ operacij #add(i,x)# in #remove(i)# porabi skupno $O(m)$ časa skozi vse klice na #resize()#.
\end{thm}

%TODO: Discuss the use of bitwise-and as a replacement for the mod operator

\section{#ArrayDeque#: Hitra obojestranska vrsta z uporabo polja}
\seclabel{arraydeque}

\index{ArrayDeque@#ArrayDeque#}%
Struktura #ArrayQueue# iz prejšnjega poglavja je podatkovna struktura za predstavitev zaporedja, ki omogoča učinkovito dodajanje na en konec, in odstranjevanje z drugega konca.  Podatkovna struktura #ArrayDeque# pa omogoče tako učinkovito dodajanje kot tudi odstranjevanje z obeh koncev.  Ta struktura implementira vmesnik #List# z upodabo enake tehnike krožnega polja, ki je uporabljena pri #ArrayQueue#.

\codeimport{ods/ArrayDeque.a.j.n}

Operaciji #get(i)# in #set(i,x)# nad #ArrayDeque# sta enostavni.  Vrneta oziroma nastavita element polja $#a[#{#(j+i)#\bmod#a.length#}#]#$.

\codeimport{ods/ArrayDeque.get(i).set(i,x)}

Implementacija operacije #add(i,x)# je bolj zanimiva.  Kot ponavadi, najprej preverimo če je #a# poln in po potrebi povečamo #a# s klicem #resize()#.  Želimo, da je ta operacija hitra tako ko je #i# majhen (blizu 0), kot tudi ko je #i# velik (blizu #n#).  Zato preverimo, če drži $#i#<#n#/2$.  Če drži, zamaknemo elemente $#a[0]#,\ldots,#a[i-1]#$ za eno mesto v levo.  Sicer ($#i#\ge#n#/2$), elemente $#a[i]#,\ldots,#a[n-1]#$ zamaknemo za eno mesto v desno.  \figref{arraydeque} prikazuje operaciji #add(i,x)# in #remove(x)# nad #ArrayDeque#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraydeque}
  \end{center}
  \caption[Dodajanje in odstranjevanje iz ArrayDeque]{Zaporedje operacij #add(i,x)# in #remove(i)# nad #ArrayDeque#. Puščice označujejo prestavljanje elementov.}
  \figlabel{arraydeque}
\end{figure}


\codeimport{ods/ArrayDeque.add(i,x)}

By doing the shifting in this way, we guarantee that #add(i,x)# never
has to shift more than $\min\{ #i#, #n#-#i# \}$ elements.  Thus, the running
time of the #add(i,x)# operation (ignoring the cost of a #resize()#
operation) is $O(1+\min\{#i#,#n#-#i#\})$.

The implementation of the #remove(i)# operation is similar.  It either
shifts elements $#a[0]#,\ldots,#a[i-1]#$ right by one position or shifts
the elements $#a[i+1]#,\ldots,#a[n-1]#$ left by one position depending
on whether $#i#<#n#/2$.  Again, this means that #remove(i)# never spends
more than $O(1+\min\{#i#,#n#-#i#\})$ time to shift elements.

\codeimport{ods/ArrayDeque.remove(i)}

\subsection{Summary}

The following theorem summarizes the performance of the #ArrayDeque#
data structure:
\begin{thm}\thmlabel{arraydeque}
  An #ArrayDeque# implements the #List# interface.  Ignoring the cost of
  calls to #resize()#, an #ArrayDeque# supports the operations
  \begin{itemize}
    \item #get(i)# and #set(i,x)# in $O(1)$ time per operation; and
    \item #add(i,x)# and #remove(i)# in $O(1+\min\{#i#,#n#-#i#\})$ time
          per operation.
  \end{itemize}
  Furthermore, beginning with an empty #ArrayDeque#, performing any
  sequence of $m$ #add(i,x)# and #remove(i)# operations results in a
  total of $O(m)$ time spent during all calls to #resize()#.
\end{thm}

\section{#DualArrayDeque#: Building a Deque from Two Stacks}
\seclabel{dualarraydeque}

\index{DualArrayDeque@#DualArrayDeque#}%
Next, we present a data structure, the #DualArrayDeque# that
achieves the same performance bounds as an #ArrayDeque# by using
two #ArrayStack#s.  Although the asymptotic performance of the
#DualArrayDeque# is no better than that of the #ArrayDeque#, it is
still worth studying, since it offers a good example of how to make a
sophisticated data structure by combining two simpler data structures.

A #DualArrayDeque# represents a list using two #ArrayStack#s.  Recall that
an #ArrayStack# is fast when the operations on it modify elements near
the end.  A #DualArrayDeque# places two #ArrayStack#s, called #front#
and #back#, back-to-back so that operations are fast at either end.

\codeimport{ods/DualArrayDeque.front.back}

A #DualArrayDeque# does not explicitly store the number, #n#,
of elements it contains.  It doesn't need to, since it contains
$#n#=#front.size()# + #back.size()#$ elements.  Nevertheless, when
analyzing the #DualArrayDeque# we will still use #n# to denote the number
of elements it contains.

\codeimport{ods/DualArrayDeque.size()}

The #front# #ArrayStack# stores the list elements that whose indices
are $0,\ldots,#front.size()#-1$, but stores them in reverse order.
The #back# #ArrayStack# contains list elements with indices
in $#front.size()#,\ldots,#size()#-1$ in the normal order.  In this way,
#get(i)# and #set(i,x)# translate into appropriate calls to #get(i)#
or #set(i,x)# on either #front# or #back#, which take $O(1)$ time per operation.

\codeimport{ods/DualArrayDeque.get(i).set(i,x)}

Note that if an index $#i#<#front.size()#$, then it corresponds to the
element of #front# at position $#front.size()#-#i#-1$, since the
elements of #front# are stored in reverse order.

Adding and removing elements from a #DualArrayDeque# is illustrated in
\figref{dualarraydeque}.  The #add(i,x)# operation manipulates either #front#
or #back#, as appropriate:

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/dualarraydeque}
  \end{center}
  \caption[Adding and removing in a DualArrayDeque]{A sequence of #add(i,x)# and #remove(i)# operations on a
  #DualArrayDeque#.  Arrows denote elements being copied.  Operations that
  result in a rebalancing by #balance()# are marked with an asterisk.}
  \figlabel{dualarraydeque}
\end{figure}



\codeimport{ods/DualArrayDeque.add(i,x)}

The #add(i,x)# method performs rebalancing of the two #ArrayStack#s
#front# and #back#, by calling the #balance()# method.  The
implementation of #balance()# is described below, but for now it is
sufficient to know that #balance()# ensures that, unless $#size()#<2$,
#front.size()# and #back.size()# do not differ by more than a factor
of 3.  In particular, $3\cdot#front.size()# \ge #back.size()#$ and
$3\cdot#back.size()# \ge #front.size()#$.

Next we analyze the cost of #add(i,x)#, ignoring the cost of calls to
#balance()#. If $#i#<#front.size()#$, then #add(i,x)# gets implemented
by the call to $#front.add(front.size()-i-1,x)#$.  Since #front# is an
#ArrayStack#, the cost of this is
\begin{equation}
  O(#front.size()#-(#front.size()#-#i#-1)+1) = O(#i#+1) \enspace .
  \eqlabel{das-front}
\end{equation}
On the other hand, if $#i#\ge#front.size()#$, then #add(i,x)# gets
implemented as $#back.add(i-front.size(),x)#$.  The cost of this is
\begin{equation}
  O(#back.size()#-(#i#-#front.size()#)+1) = O(#n#-#i#+1) \enspace .
  \eqlabel{das-back}
\end{equation}

Notice that the first case \myeqref{das-front} occurs when $#i#<#n#/4$.
The second case \myeqref{das-back} occurs when $#i#\ge 3#n#/4$.  When
$#n#/4\le#i#<3#n#/4$, we cannot be sure whether the operation affects
#front# or #back#, but in either case, the operation takes
$O(#n#)=O(#i#)=O(#n#-#i#)$ time, since $#i#\ge #n#/4$ and $#n#-#i#>
#n#/4$.  Summarizing the situation, we have
\[
     \mbox{Running time of } #add(i,x)# \le 
          \left\{\begin{array}{ll}
            O(1+ #i#) & \mbox{if $#i#< #n#/4$} \\
            O(#n#) & \mbox{if $#n#/4 \le #i# < 3#n#/4$} \\
            O(1+#n#-#i#) & \mbox{if $#i# \ge 3#n#/4$}
          \end{array}\right.
\]
Thus, the running time of #add(i,x)#, if we ignore the cost of the call
to #balance()#, is $O(1+\min\{#i#, #n#-#i#\})$.

The #remove(i)# operation and its analysis resemble the #add(i,x)#
operation and analysis.

\codeimport{ods/DualArrayDeque.remove(i)}

\subsection{Balancing}

Finally, we turn to the #balance()# operation performed by #add(i,x)#
and #remove(i)#.  This operation ensures that neither #front# nor #back#
becomes too big (or too small).  It ensures that, unless there are fewer
than two elements, each of #front# and #back# contain at least $#n#/4$
elements. If this is not the case, then it moves elements between them
so that #front# and #back# contain exactly $\lfloor#n#/2\rfloor$ elements
and $\lceil#n#/2\rceil$ elements, respectively.

\codeimport{ods/DualArrayDeque.balance()}

Here there is little to analyze.  If the #balance()# operation does
rebalancing, then it moves $O(#n#)$ elements and this takes $O(#n#)$
time. This is bad, since #balance()# is called with each call to
#add(i,x)# and #remove(i)#.  However, the following lemma shows that, on
average, #balance()# only spends a constant amount of time per operation.

\begin{lem}\lemlabel{dualarraydeque-amortized}
  If an empty #DualArrayDeque# is created and any sequence of $m\ge 1$ calls
  to #add(i,x)# and #remove(i)# are performed, then the total time spent
  during all calls to #balance()# is $O(m)$.
\end{lem}

\begin{proof}
  We will show that, if #balance()# is forced to shift elements, then
  the number of #add(i,x)# and #remove(i)# operations since the last
  time any elements were shifted by #balance()# is at least $#n#/2-1$.
  As in the proof of \lemref{arraystack-amortized}, this is sufficient
  to prove that the total time spent by #balance()# is $O(m)$.

  We will perform our analysis using a technique knows as the
  \emph{potential method}.
  \index{potential}%
  \index{potential method}%
  Define the \emph{potential}, $\Phi$, of the
  #DualArrayDeque# as the difference in size between #front# and #back#:
  \[  \Phi = |#front.size()# - #back.size()#| \enspace . \]
  The interesting thing about this potential is that a call to #add(i,x)#
  or #remove(i)# that does not do any balancing can increase the potential
  by at most 1.

  Observe that, immediately after a call to #balance()# that shifts
  elements, the potential, $\Phi_0$, is at most 1, since
  \[ \Phi_0 = \left|\lfloor#n#/2\rfloor-\lceil#n#/2\rceil\right|\le 1  \enspace .\]

  Consider the situation immediately before a call to #balance()# that
  shifts elements and suppose, without loss of generality, that #balance()#
  is shifting elements because $3#front.size()# < #back.size()#$.
  Notice that, in this case,
  \begin{eqnarray*}
   #n# & = & #front.size()#+#back.size()# \\
       & < & #back.size()#/3+#back.size()# \\
       & = & \frac{4}{3}#back.size()#
  \end{eqnarray*}
  Furthermore, the potential at this point in time is
  \begin{eqnarray*}
  \Phi_1 & = & #back.size()# - #front.size()# \\
      &>& #back.size()# - #back.size()#/3 \\
      &=& \frac{2}{3}#back.size()# \\
      &>& \frac{2}{3}\times\frac{3}{4}#n# \\
      &=& #n#/2
  \end{eqnarray*}
  Therefore, the number of calls to #add(i,x)# or #remove(i)# since
  the last time #balance()# shifted elements is at least $\Phi_1-\Phi_0
  > #n#/2-1$. This completes the proof.
\end{proof}

\subsection{Summary}

The following theorem summarizes the properties of a #DualArrayDeque#:

\begin{thm}\thmlabel{dualarraydeque}
  A #DualArrayDeque# implements the #List# interface.  Ignoring the
  cost of calls to #resize()# and #balance()#, a #DualArrayDeque#
  supports the operations
  \begin{itemize}
    \item #get(i)# and #set(i,x)# in $O(1)$ time per operation; and
    \item #add(i,x)# and #remove(i)# in $O(1+\min\{#i#,#n#-#i#\})$ time
          per operation.
  \end{itemize}
  Furthermore, beginning with an empty #DualArrayDeque#, any sequence of $m$
  #add(i,x)# and #remove(i)# operations results in a total of $O(m)$
  time spent during all calls to #resize()# and #balance()#.
\end{thm}


\section{#RootishArrayStack#: A Space-Efficient Array Stack}
\seclabel{rootisharraystack}

\index{RootishArrayStack@#RootishArrayStack#}%
One of the drawbacks of all previous data structures in this chapter is
that, because they store their data in one or two arrays and they avoid
resizing these arrays too often, the arrays frequently are not very full.
For example, immediately after a #resize()# operation on an #ArrayStack#,
the backing array #a# is only half full.  Even worse, there are times
when only $1/3$ of #a# contains data.

In this section, we discuss the #RootishArrayStack# data structure,
that addresses the problem of wasted space.  The #RootishArrayStack#
stores #n# elements using $O(\sqrt{#n#})$ arrays.  In these arrays, at
most $O(\sqrt{#n#})$ array locations are unused at any time.  All
remaining array locations are used to store data.  Therefore, these
data structures waste at most $O(\sqrt{#n#})$ space when storing #n#
elements.

A #RootishArrayStack# stores its elements in a list of #r#
arrays called \emph{blocks} that are numbered $0,1,\ldots,#r#-1$.
See \figref{rootisharraystack}.  Block $b$ contains $b+1$ elements.
Therefore, all #r# blocks contain a total of
\[
  1+ 2+ 3+\cdots +#r# = #r#(#r#+1)/2
\]
elements.  The above formula can be obtained as shown in \figref{gauss}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/rootisharraystack}
  \end{center}
  \caption[Adding and removing in a RootishArrayStack]{A sequence of #add(i,x)# and #remove(i)# operations on a
  #RootishArrayStack#.  Arrows denote elements being copied. }
  \figlabel{rootisharraystack}
\end{figure}

\codeimport{ods/RootishArrayStack.blocks.n}

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/gauss}
  \end{center}
  \caption{The number of white squares is $1+2+3+\cdots+#r#$.  The number of
  shaded squares is the same.  Together the white and shaded squares make a
  rectangle consisting of $#r#(#r#+1)$ squares.}
  \figlabel{gauss}
\end{figure}

As we might expect, the elements of the list are laid out in order
within the blocks.  The list element with index 0 is stored in block 0,
elements with list indices 1 and 2 are stored in block 1, elements with
list indices 3, 4, and 5 are stored in block 2, and so on.  The main
problem we have to address is that of determining, given an index $#i#$,
which block contains #i# as well as the index corresponding to #i#
within that block.

Determining the index of #i# within its block turns out to be easy. If
index #i# is in block #b#, then the number of elements in blocks
$0,\ldots,#b#-1$ is $#b#(#b#+1)/2$.  Therefore, #i# is stored at location
\[
     #j# = #i# - #b#(#b#+1)/2
\]
within block #b#.  Somewhat more challenging is the problem of determining
the value of #b#.  The number of elements that have indices less than
or equal to #i# is $#i#+1$.  On the other hand, the number of elements
in blocks 0,\ldots,b is $(#b#+1)(#b#+2)/2$.  Therefore, #b# is the smallest
integer such that
\[
    (#b#+1)(#b#+2)/2 \ge #i#+1 \enspace .
\]
We can rewrite this equation as
\[
    #b#^2 + 3#b# - 2#i# \ge  0 \enspace .
\]
The corresponding quadratic equation $#b#^2 + 3#b# - 2#i# =  0$ has two
solutions: $#b#=(-3 + \sqrt{9+8#i#}) / 2$ and $#b#=(-3 - \sqrt{9+8#i#}) / 2$.
The second solution makes no sense in our application since it always
gives a negative value. Therefore, we obtain the solution $#b# = (-3 +
\sqrt{9+8i}) / 2$.  In general, this solution is not an integer, but
going back to our inequality, we want the smallest integer $#b#$ such that 
$#b# \ge (-3 + \sqrt{9+8i}) / 2$.  This is simply
\[
   #b# = \left\lceil(-3 + \sqrt{9+8i}) / 2\right\rceil \enspace .
\]

\codeimport{ods/RootishArrayStack.i2b(i)}

With this out of the way, the #get(i)# and #set(i,x)# methods are straightforward.  We first compute the appropriate block #b# and the appropriate index #j# within the block and then perform the appropriate operation:

\codeimport{ods/RootishArrayStack.get(i).set(i,x)}

If we use any of the data structures in this chapter for representing the #blocks# list, then #get(i)# and #set(i,x)# will each run in constant time.

The #add(i,x)# method will, by now, look familiar.  We first check
to see if our data structure is full, by checking if the number of
blocks #r# is such that $#r#(#r#+1)/2 = #n#$. If so, we call #grow()#
to add another block.  With this done, we shift elements with indices
$#i#,\ldots,#n#-1$ to the right by one position to make room for the
new element with index #i#:

\codeimport{ods/RootishArrayStack.add(i,x)}

The #grow()# method does what we expect. It adds a new block:

\codeimport{ods/RootishArrayStack.grow()}

Ignoring the cost of the #grow()# operation, the cost of an #add(i,x)#
operation is dominated by the cost of shifting and is therefore
$O(1+#n#-#i#)$, just like an #ArrayStack#.

The #remove(i)# operation is similar to #add(i,x)#.  It shifts the
elements with indices $#i#+1,\ldots,#n#$ left by one position and then,
if there is more than one empty block, it calls the #shrink()# method
to remove all but one of the unused blocks:

\codeimport{ods/RootishArrayStack.remove(i)}
\codeimport{ods/RootishArrayStack.shrink()}

Once again, ignoring the cost of the #shrink()# operation, the cost of
a #remove(i)# operation is dominated by the cost of shifting  and is
therefore $O(#n#-#i#)$.

\subsection{Analysis of Growing and Shrinking}

The above analysis of #add(i,x)# and #remove(i)# does not account for
the cost of #grow()# and #shrink()#.  Note that, unlike the
#ArrayStack.resize()# operation, #grow()# and #shrink()# do not copy
any data.  They only allocate or free an array of size #r#.  In
some environments, this takes only constant time, while in others, it
may require time proportional to #r#.

We note that, immediately after a call to #grow()# or #shrink()#, the
situation is clear. The final block is completely empty, and all other
blocks are completely full.  Another call to #grow()# or #shrink()# will
not happen until at least $#r#-1$ elements have been added or removed.
Therefore, even if #grow()# and #shrink()# take $O(#r#)$ time, this
cost can be amortized over at least $#r#-1$ #add(i,x)# or #remove(i)#
operations, so that the amortized cost of #grow()# and #shrink()# is
$O(1)$ per operation.

\subsection{Space Usage}
\seclabel{rootishspaceusage}

Next, we analyze the amount of extra space used by a #RootishArrayStack#.
In particular, we want to count any space used by a #RootishArrayStack# that is not an array element currently used to hold a list element.  We call all such space \emph{wasted space}.
\index{wasted space}%

The #remove(i)# operation ensures that a #RootishArrayStack# never has
more than two blocks that are not completely full.  The number of blocks,
#r#, used by a #RootishArrayStack# that stores #n# elements therefore
satisfies
\[
    (#r#-2)(#r#-1) \le #n# \enspace .
\]
Again, using the quadratic equation on this gives
\[
   #r# \le (3+\sqrt{1+4#n#})/2 = O(\sqrt{#n#}) \enspace .
\]
The last two blocks have sizes #r# and #r-1#, so the space wasted by these
two blocks is at most $2#r#-1 = O(\sqrt{#n#})$.  If we store the blocks
in (for example) an #ArrayList#, then the amount of space wasted by the
#List# that stores those #r# blocks is also $O(#r#)=O(\sqrt{#n#})$.  The
other space needed for storing #n# and other accounting information is $O(1)$.
Therefore, the total amount of wasted space in a #RootishArrayStack#
is $O(\sqrt{#n#})$.

Next, we argue that this space usage is optimal for any data structure
that starts out empty and can support the addition of one item at
a time. More precisely, we will show that, at some point during the
addition of #n# items, the data structure is wasting an amount of space
at least in $\sqrt{#n#}$ (though it may be only wasted for a moment).

Suppose we start with an empty data structure and we add #n# items one
at a time.  At the end of this process, all #n# items are stored in
the structure and distributed among a collection of #r# memory blocks.
If $#r#\ge \sqrt{#n#}$, then the data structure must be using #r#
pointers (or references) to keep track of these #r# blocks, and these
pointers are wasted space.  On the other hand, if $#r# < \sqrt{#n#}$
then, by the pigeonhole principle, some block must have a size of at
least $#n#/#r# > \sqrt{#n#}$.  Consider the moment at which this block
was first allocated.  Immediately after it was allocated, this block
was empty, and was therefore wasting $\sqrt{#n#}$ space.  Therefore,
at some point in time during the insertion of #n# elements, the data
structure was wasting $\sqrt{#n#}$ space.

\subsection{Summary}

The following theorem summarizes our discussion of the #RootishArrayStack#
data structure:

\begin{thm}\thmlabel{rootisharraystack}
  A #RootishArrayStack# implements the #List# interface.  Ignoring the cost of
  calls to #grow()# and #shrink()#, a #RootishArrayStack# supports the operations
  \begin{itemize}
    \item #get(i)# and #set(i,x)# in $O(1)$ time per operation; and
    \item #add(i,x)# and #remove(i)# in $O(1+#n#-#i#)$ time per operation.
  \end{itemize}
  Furthermore, beginning with an empty #RootishArrayStack#, any sequence of $m$
  #add(i,x)# and #remove(i)# operations results in a total of $O(m)$
  time spent during all calls to #grow()# and #shrink()#.

  The space (measured in words)\footnote{Recall \secref{model} for a
  discussion of how memory is measured.} used by a #RootishArrayStack#
  that stores #n# elements is $#n# +O(\sqrt{#n#})$.
\end{thm}

\subsection{Computing Square Roots}

\index{square roots}%
A reader who has had some exposure to models of computation may notice
that the #RootishArrayStack#, as described above, does not fit into
the usual word-RAM model of computation (\secref{model}) because it
requires taking square roots.  The square root operation is generally
not considered a basic operation and is therefore not usually part of
the word-RAM model.

In this section, we show that the square root operation can be
implemented efficiently.  In particular, we show that for any integer
$#x#\in\{0,\ldots,#n#\}$,  $\lfloor\sqrt{#x#}\rfloor$ can be computed
in constant-time, after $O(\sqrt{#n#})$ preprocessing that creates two
arrays of length $O(\sqrt{#n#})$.  The following lemma shows that we
can reduce the problem of computing the square root of #x# to the square
root of a related value #x'#.

\begin{lem}\lemlabel{root}
Let $#x#\ge 1$ and let $#x'#=#x#-a$, where $0\le a\le\sqrt{#x#}$.  Then
   $\sqrt{x'} \ge \sqrt{#x#}-1$.
\end{lem}

\begin{proof}
It suffices to show that
\[
\sqrt{#x#-\sqrt{#x#}} \ge \sqrt{#x#}-1 \enspace .
\]
Square both sides of this inequality to get
\[
 #x#-\sqrt{#x#} \ge #x#-2\sqrt{#x#}+1
\]
and gather terms to get 
\[
 \sqrt{#x#} \ge 1
\]
which is clearly true for any $#x#\ge 1$.
\end{proof}

Start by restricting the problem a little, and assume that $2^{#r#} \le
#x# < 2^{#r#+1}$, so that $\lfloor\log #x#\rfloor=#r#$, i.e., #x# is an
integer having $#r#+1$ bits in its binary representation.  We can take
$#x'#=#x# - (#x#\bmod 2^{\lfloor r/2\rfloor})$.  Now, #x'# satisfies
the conditions of \lemref{root}, so $\sqrt{#x#}-\sqrt{#x'#} \le 1$.
Furthermore, #x'# has all of its lower-order $\lfloor #r#/2\rfloor$ bits
equal to 0, so there are only
\[
  2^{#r#+1-\lfloor #r#/2\rfloor} \le 4\cdot2^{#r#/2} \le 4\sqrt{#x#}
\]
possible values of #x'#.  This means that we can use an array, #sqrttab#,
that stores the value of $\lfloor\sqrt{#x'#}\rfloor$ for each possible
value of #x'#.  A little more precisely, we have
\[
   #sqrttab#[i] 
    = \left\lfloor
       \sqrt{i 2^{\lfloor #r#/2\rfloor}}
      \right\rfloor \enspace .
\]
In this way, $#sqrttab#[i]$ is within 2 of $\sqrt{#x#}$ for all
$#x#\in\{i2^{\lfloor r/2\rfloor},\ldots,(i+1)2^{\lfloor r/2\rfloor}-1\}$.
Stated another way, the array entry 
$#s#=#sqrttab#[#x##>>#\lfloor #r#/2\rfloor]$ is either equal to
$\lfloor\sqrt{#x#}\rfloor$,
$\lfloor\sqrt{#x#}\rfloor-1$, or
$\lfloor\sqrt{#x#}\rfloor-2$.  From #s# we can determine the value
of $\lfloor\sqrt{#x#}\rfloor$ by
incrementing #s# until 
$(#s#+1)^2 > #x#$.
\codeimport{ods/FastSqrt.sqrt(x,r)}

Now, this only works for $#x#\in\{2^{#r#},\ldots,2^{#r#+1}-1\}$ and
#sqrttab# is a special table that only works for a particular value
of $#r#=\lfloor\log #x#\rfloor$.  To overcome this, we could compute
$\lfloor\log #n#\rfloor$ different #sqrttab# arrays, one for each possible
value of $\lfloor\log #x#\rfloor$. The sizes of these tables form an exponential sequence whose largest value is at most $4\sqrt{#n#}$, so the total size of all tables is $O(\sqrt{#n#})$.

However, it turns out that more than one #sqrttab# array is unnecessary;
we only need one #sqrttab# array for the value $#r#=\lfloor\log
#n#\rfloor$.  Any value #x# with $\log#x#=#r'#<#r#$ can be \emph{upgraded}
by multiplying #x# by $2^{#r#-#r'#}$ and using the equation
\[
    \sqrt{2^{#r#-#r'#}x} = 2^{(#r#-#r#')/2}\sqrt{#x#} \enspace .
\]
The quantity $2^{#r#-#r#'}x$ is in the range
$\{2^{#r#},\ldots,2^{#r#+1}-1\}$ so we can look up its square root
in #sqrttab#.  The following code implements this idea to compute
$\lfloor\sqrt{#x#}\rfloor$ for all non-negative integers #x# in the
range $\{0,\ldots,2^{30}-1\}$ using an array, #sqrttab#, of size $2^{16}$.
\codeimport{ods/FastSqrt.sqrt(x)}

Something we have taken for granted thus far is the question of how
to compute
$#r#'=\lfloor\log#x#\rfloor$.  Again, this is a problem that can be solved
with an array, #logtab#, of size $2^{#r#/2}$.  In this case, the
code is particularly simple, since $\lfloor\log #x#\rfloor$ is just the
index of the most significant 1 bit in the binary representation of #x#.
This means that, for $#x#>2^{#r#/2}$, we can right-shift the bits of
#x# by $#r#/2$ positions before using it as an index into #logtab#.
The following code does this using an array #logtab# of size $2^{16}$ to compute
$\lfloor\log #x#\rfloor$ for all #x# in the range $\{1,\ldots,2^{32}-1\}$.
\codeimport{ods/FastSqrt.log(x)}

Finally, for completeness, we include the following code that initializes #logtab# and #sqrttab#:
\codeimport{ods/FastSqrt.inittabs()}

To summarize, the computations done by the #i2b(i)# method can be
implemented in constant time on the word-RAM using $O(\sqrt{n})$ extra
memory to store the #sqrttab# and #logtab# arrays.  These arrays can be
rebuilt when #n# increases or decreases by a factor of two, and the cost
of this rebuilding can be amortized over the number of #add(i,x)# and
#remove(i)# operations that caused the change in #n# in the same way that
the cost of #resize()# is analyzed in the #ArrayStack# implementation.


\section{Discussion and Exercises}

Most of the data structures described in this chapter are folklore. They
can be found in implementations dating back over 30 years.  For example,
implementations of stacks, queues, and deques, which generalize easily
to the #ArrayStack#, #ArrayQueue# and #ArrayDeque# structures described
here, are discussed by Knuth \cite[Section~2.2.2]{k97v1}.

Brodnik \etal\ \cite{bcdms99} seem to have been the first to describe
the #RootishArrayStack# and prove a $\sqrt{n}$ lower-bound like that
in \secref{rootishspaceusage}.  They also present a different structure
that uses a more sophisticated choice of block sizes in order to avoid
computing square roots in the #i2b(i)# method.  Within their scheme,
the block containing #i# is block $\lfloor\log (#i#+1)\rfloor$, which
is simply the index of the leading 1 bit in the binary representation
of $#i#+1$.  Some computer architectures provide an instruction for
computing the index of the leading 1-bit in an integer. \javaonly{In
Java, the #Integer# class provides a method #numberOfLeadingZeros(i)#
from which one can easily compute $\lfloor\log (#i#+1)\rfloor$.}

A structure related to the #RootishArrayStack# is the two-level
\emph{tiered-vector} of Goodrich and Kloss \cite{gk99}.
\index{tiered-vector}%
This structure
supports the #get(i,x)# and #set(i,x)# operations in constant time and
#add(i,x)# and #remove(i)# in $O(\sqrt{#n#})$ time.  These running times
are similar to what can be achieved with the more careful implementation
of a #RootishArrayStack# discussed in \excref{rootisharraystack-fast}.

\javaonly{
\begin{exc}
  In the #ArrayStack# implementation, after the first call to #remove(i)#,
  the backing array, #a#, contains $#n#+1$ non-#null# values despite
  the fact that the #ArrayStack# only contains #n# elements.  Where is
  the extra non-#null# value?  Discuss any consequences this non-#null#
  value might have on the Java Runtime Environment's memory manager.
  \index{Java Runtime Environment}%
  \index{memory manager}%
\end{exc}
}

\begin{exc}
  The #List# method #addAll(i,c)# inserts all elements of the #Collection#
  #c# into the list at position #i#.  (The #add(i,x)# method is a special
  case where $#c#=\{#x#\}$.)  Explain why, for the data structures
  in this chapter, it is not efficient to implement #addAll(i,c)# by
  repeated calls to #add(i,x)#.  Design and implement a more efficient
  implementation.
\end{exc}

\begin{exc}
  Design and implement a \emph{#RandomQueue#}.
  \index{RandomQueue@#RandomQueue#}%
  This is an implementation
  of the #Queue# interface in which the #remove()# operation removes
  an element that is chosen uniformly at random among all the elements
  currently in the queue.  (Think of a #RandomQueue# as a bag in which
  we can add elements or reach in and blindly remove some random element.)
  The #add(x)# and #remove()# operations in a #RandomQueue# should run
  in constant time per operation.
\end{exc}

\begin{exc}
  Design and implement a #Treque# (triple-ended queue). 
  \index{Treque@#Treque#}%
  This is a #List#
  implementation in which #get(i)# and #set(i,x)# run in constant time
  and #add(i,x)# and #remove(i)# run in time
  \[
     O(1+\min\{#i#, #n#-#i#, |#n#/2-#i#|\}) \enspace .
  \]
  In other words, modifications are fast if they are near either
  end or near the middle of the list.
\end{exc}

\begin{exc}
  Implement a method #rotate(a,r)# that ``rotates'' the array #a#
  so that #a[i]# moves to $#a#[(#i#+#r#)\bmod #a.length#]$, for all
  $#i#\in\{0,\ldots,#a.length#\}$.
\end{exc}

\begin{exc}
  Implement a method #rotate(r)# that ``rotates'' a #List# so that
  list item #i# becomes list item $(#i#+#r#)\bmod #n#$.  When run on
  an #ArrayDeque#, or a #DualArrayDeque#, #rotate(r)# should run in
  $O(1+\min\{#r#,#n#-#r#\})$ time.
\end{exc}

\begin{exc}
  Modify the #ArrayDeque# implementation so that the shifting
  done by #add(i,x)#, #remove(i)#, and #resize()# is done using
  the faster #System.arraycopy(s,i,d,j,n)# method.
\end{exc}

\begin{exc}
  Modify the #ArrayDeque# implementation so that it does not use the
  #%# operator (which is expensive on some systems).  Instead, it
  should make use of the fact that, if #a.length# is a power of 2,
  then 
  \[  #k%a.length#=#k&(a.length-1)# \enspace .
  \]
  (Here, #&# is the bitwise-and operator.)
\end{exc}

\begin{exc}
  Design and implement a variant of #ArrayDeque# that does not do any
  modular arithmetic at all.  Instead, all the data sits in a consecutive
  block, in order, inside an array.  When the data overruns the beginning
  or the end of this array, a modified #rebuild()# operation is performed.
  The amortized cost of all operations should be the same as in an
  #ArrayDeque#.

  \noindent Hint: Getting this to work is really all about how you implement
  the #rebuild()# operation.  You would like #rebuild()# to put the data
  structure into a state where the data cannot run off either end until
  at least $#n#/2$ operations have been performed.

  Test the performance of your implementation against the #ArrayDeque#.
  Optimize your implementation (by using #System.arraycopy(a,i,b,i,n)#)
  and see if you can get it to outperform the #ArrayDeque# implementation.
\end{exc}

\begin{exc}
  Design and implement a version of a #RootishArrayStack# that has
  only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)#
  and #remove(i,x)# operations in $O(1+\min\{#i#,#n#-#i#\})$ time.
\end{exc}

\begin{exc}\exclabel{rootisharraystack-fast}
  Design and implement a version of a #RootishArrayStack# that has
  only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)#
  and #remove(i,x)# operations in $O(1+\min\{\sqrt{#n#},#n#-#i#\})$
  time. (For an idea on how to do this, see \secref{selist}.)
\end{exc}

\begin{exc}
  Design and implement a version of a #RootishArrayStack# that has
  only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)# and
  #remove(i,x)# operations in $O(1+\min\{#i#,\sqrt {#n#},#n#-#i#\})$ time.
  (See \secref{selist} for ideas on how to achieve this.)
\end{exc}

\begin{exc}
  Design and implement a #CubishArrayStack#.
  \index{CubishArrayStack@#CubishArrayStack#}%
  This three level structure
  implements the #List# interface using $O(n^{2/3})$ wasted space.
  In this structure, #get(i)# and #set(i,x)# take constant time; while
  #add(i,x)# and #remove(i)# take $O(#n#^{1/3})$ amortized time.
\end{exc}


