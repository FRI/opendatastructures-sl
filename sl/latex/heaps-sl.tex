\chapter{Kopice}
\chaplabel{Kopice}
\translatedby{David Zavodnik, Grega Vertovšek, Luka Vabič, Luka Florjančič, Jure Žvelc, Rok Komatar}{sl}

V tem poglavju si bomo pogledali 2 implementacije zelo uporabne podatkovne strukture
#Polje# s prednostjo. Obe od teh dveh struktur sta posebne oblike Dvojiškega drevesa imenovani \emph{Kopica}, 
\index{Kopica}%
\index{Binarna Kopica}%
\index{Kopica!Binarna}%
kar pomeni ''neorganizirana kopica''. To je v nasprotju z dvojiškimi iskalnimi drevesi 
pri katerih pomislimo na zelo urejeno kopico.

Prva izvedba kopic uporablja polje, da simuliramo popolno dvojiško drevo. Ta zelo hitra implementacija je osnova za enega izmed najhitrejših znanih sortirnih algoritmov, in sicer kopično urejanje (glej \secref{heapsort}).
Druga implementacija je bazirana na bolj fleksiblinih dvojiških drevesih, ki podpirajo #meld(h)# operacijo, ki omogoča vrsti s prednostjo, da obsorbira elemente druge vrste s prednostjo #h#.

\section{#BinaryHeap#: implicitno dvojiško drevo}
\seclabel{binaryheap}

\index{BinarnaKopica@#BinaryHeap#}%
Naša prva implementacija #Queue# (s prednostjo) temelji na tehniki, 
ki je stara preko 400 let. \emph{Eytzingerjeva metoda}
\index{Eytzingerjeva metoda}%
nam omogoča, da predstavimo popolno dvojiško drevo kot polje, v katerem imamo vozlišča 
postavljena v vrsto iz leve proti desni (glej \secref{bintree:traversal}).
Na ta način je koren drevesa shranjen na poziciji 0, njegov levi otrok je shranjen na poziciji 0, 
njegov desni otrok na pozciji 1, levi otrok na 2, levi otrok otroka na poziciji 3 in tako naprej.
Glej \figref{eytzinger}.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/eytzinger}
  \end{center}
  \caption{Eytzingerjeva metoda predstavlja popolno dvojiško drevo kot polje.}
  \figlabel{eytzinger}
\end{figure}

Če uporabimo Eytzingerjevo metodo na dovolj velikih drevesih se za-čnejo pojavljati vzorci.
Levi otrok vozlišča pri indexu #i# je na indexu $#left(i)#=2#i#+1$ 
in desni otrok vozlišča pri indexu #i# je na indexu $#right(i)#=2#i#+2$.
Starš vozlišča pri indexu #i# pa je na $#parent(i)#=(#i#-1)/2$.
\codeimport{ods/BinaryHeap.left(i).right(i).parent(i)}

#BinaryHeap# uporablja to tehniko, da implicitno predstavi popolno dvojiško drevo
v katerem so elementi \emph{kopično urejeni}:
\index{heap-ordered binary tree}%
\index{binary tree!heap-ordered}%
\index{heap order}%
Vrednost shranjena na katerem koli indexu #i# ni manjša kot vrednost shranjena
na katerem koli indexu #parent(i)#, razen izjeme vrednosti korena $#i#=0$. To nam 
o-mogoča, da je najmanjša vrednost #Queue# s prednostjo tako shranjena na poziciji 0 (koren).

V #BinaryHeap#,  je #n# elementov shranjenih v tabeli #a#:
\codeimport{ods/BinaryHeap.a.n}

Implementacija operacije #add(x)# je preprosta. Kot vse strukture bazirane na polju
najprej pogledamo, če je #a# poln (preverimo $#a.length#=#n#$) in če je, povečamo #a#. Nato #x# zapišemo na mesto #a[n]# in povečamo #n#. Na tej točki je potrebno storiti samo še to, da zagotovimo lastnost kopice.
To storimo tako, da zamenjujemo #x# z njegovim staršem, dokler ni #x# manjši od svojega starša.
Glej \figref{heap-insert}.
\codeimport{ods/BinaryHeap.add(x).bubbleUp(i)}

\begin{figure}
  \begin{center}
    \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/heap-insert-1} \\
    \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/heap-insert-2} \\
    \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/heap-insert-3} \\
    \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/heap-insert-4} \\
  \end{center}
  \caption[Adding to a BinaryHeap]{Dodajanje elementa 6 v #BinaryHeap#.}
  \figlabel{heap-insert}
\end{figure}

Implementacija #remove()# operacije, katera odstrani najmanjšo vred-nost
v kopici, je nekoliko težja. Vemo, kje je najmanjši element (v korenu), vendar ga moramo po odstranitvi nadomestiti in zagotoviti, da ohranjamo
lastnosti kopice.

Najlažji način, da to naredimo je, da koren nadomestimo z vrednostjo #a[n-1]#, 
zbrišemo vrednost in zmanjšamo #n#. Na žalost novi koren najverjetneje ni najmanjši element, zato ga  moramo prestaviti po kopici navzdol. To naredimo tako, da rekurzivno primerjamo element z njegovimi otroki. V primeru, da je element v kopici najmanjši smo končali, v nasprotnem primeru ga zamenjamo z najmanjšim izmed otrok in nadaljujemo ta postopek rekurzivno.
\codeimport{ods/BinaryHeap.remove().trickleDown(i)}

\begin{figure}
  \begin{center}
    \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/heap-remove-1} \\
    \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/heap-remove-2} \\
    \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/heap-remove-3} \\
    \includegraphics[height=\QuarterHeightScaleIfNeeded]{figs/heap-remove-4} \\
  \end{center}
  \caption[Odstranjevanje iz dvojiške kopice]{Odstranjevanje najmanjšega elementa, 4, iz #BinaryHeap#.}
  \figlabel{heap-remove}
\end{figure}


Kot ostale implementirane strukture polja, bomo mi ignorirali porabljen čas
v celicah za funkcijo #resize()#, ker se to lahko obračunava na amortizacijskem argumentu iz Lemma
\lemref{arraystack-amortized}. Pretečeni čas za #add(x)# in #remove()# je odvisen
od višine (implicitnega) dvojiškega drevesa. Na srečo je to \emph{polno}
\index{dvojiško drevo!Popolno}%
\index{popolno Dvojiško drevo}%
Dvojiško drevo;  vsak nivo, razen zadnjega ima največje možno število vozlišč. Tako, je
višina drevesa enaka $h$ in ima najmanj $2^h$ vozlišč. Začnimo na ta način
\[
  #n# \ge 2^h \enspace .
\]  
Če logaritmiramo, dobimo na obeh straneh enačbe
\[
   h \le \log #n# \enspace .
\]
Tako obe, #add(x)# in #remove()# operaciji tečeta v $O(\log #n#)$ času.

\subsection{Povzetek}

Naslednji teorem povzame uspešnost #BinaryHeap#.

\begin{thm}\thmlabel{binaryheap}
  #BinaryHeap# implementira #Queue# (s prednostjo). Ignoriramo ceno 
  polja da se poveča #resize()#, #BinaryHeap# podpira operaciji 
  #add(x)# in #remove()# v času $O(\log #n#)$ na operacijo.

  Poleg tega, začnimo s prazno #BinaryHeap#, katero koli zaporedje $m$
  #add(x)# in #remove()# je rezultat skupaj $O(m)$ čas enak
  porabljen za vse klice funkcije #resize()#.
\end{thm}

\section{#MeldableHeap#: Naključna zlivalna kopica}
\seclabel{Zlivalna kopica}

\index{ZlivalnaKopica@#MeldableHeap#}%
V poglavju bomo opisali #MeldableHeap#, implementacijo prioritetne vrste #Queue#, shranjeno 
v kopičasto urejenem dvojiškem drevesu. Za razliko od #BinaryHeap#, pri katerem dvojiško drevo
definira število elementov, dvojiško drevo #MeldableHeap# nima omejitev glede oblike.

The #add(x)# and #remove()# operations in a #MeldableHeap# sta blablablala ok to je delal.. ššščččžžž
implemented in terms of the #merge(h1,h2)# operation.  This operation
takes two heap nodes #h1# and #h2# and merges them, returning a heap
node that is the root of a heap that contains all elements in the subtree
rooted at #h1# and all elements in the subtree rooted at #h2#.

The nice thing about a #merge(h1,h2)# operation is that it can be
defined recursively. See \figref{meldable-merge}.  If either #h1# or
#h2# is #nil#, then we are merging with an empty set, so we return #h2#
or #h1#, respectively.  Otherwise, assume $#h1.x# \le #h2.x#$ since,
if $#h1.x# > #h2.x#$, then we can reverse the roles of #h1# and #h2#.
Then we know that the root of the merged heap will contain #h1.x#, and
we can recursively merge #h2# with #h1.left# or #h1.right#, as we wish.
This is where randomization comes in, and we toss a coin to decide
whether to merge #h2# with #h1.left# or #h1.right#:
\codeimport{ods/MeldableHeap.merge(h1,h2)}

\begin{figure}
  \centering{\includegraphics[width=\ScaleIfNeeded]{figs/meldable-merge}}
  \caption[Merging in a MeldableHeap]{Merging #h1# and #h2# is done by merging #h2# with one of
  #h1.left# or #h1.right#.}
  \figlabel{meldable-merge}
\end{figure}

In the next section, we show that #merge(h1,h2)# runs in $O(\log #n#)$
expected time, where #n# is the total number of elements in #h1# and #h2#.

With access to a #merge(h1,h2)# operation, the #add(x)# operation is easy.  We create a new node #u# containing #x# and then merge #u# with the root of our heap:
\codeimport{ods/MeldableHeap.add(x)}
This takes $O(\log (#n#+1)) = O(\log #n#)$ expected time.

The #remove()# operation is similarly easy.  The node we want to remove
is the root, so we just merge its two children and make the result the root:
\codeimport{ods/MeldableHeap.remove()}
Again, this takes $O(\log #n#)$ expected time.

Additionally, a #MeldableHeap# can implement many other operations in
$O(\log #n#)$ expected time, including:
\begin{itemize}
\item #remove(u)#: remove the node #u# (and its key #u.x#) from the heap.
\item #absorb(h)#: add all the elements of the #MeldableHeap# #h# to this heap, emptying #h# in the process.
\end{itemize}
Each of these operations can be implemented using a constant number of
#merge(h1,h2)# operations that each take $O(\log #n#)$ expected time.

\subsection{Analysis of #merge(h1,h2)#}

The analysis of #merge(h1,h2)# is based on the analysis of a random walk
in a binary tree.  A \emph{random walk} in a binary tree starts at the
root of the tree.  At each step in the random walk, a coin is tossed and,
depending on the result of this coin toss, the walk proceeds to the left
or to the right child of the current node.  The walk ends when it falls
off the tree (the current node becomes #nil#).

The following lemma is somewhat remarkable because it does not depend
at all on the shape of the binary tree:

\begin{lem}\lemlabel{tree-random-walk}
The expected length of a random walk in a binary tree with #n# nodes is at most #\log (n+1)#.
\end{lem}

\begin{proof}
The proof is by induction on #n#. In the base case, $#n#=0$ and the walk
has length $0=\log (#n#+1)$.  Suppose now that the result is true for
all non-negative integers $#n#'< #n#$.

Let $#n#_1$ denote the size of the root's left subtree, so that
$#n#_2=#n#-#n#_1-1$ is the size of the root's right subtree.  Starting at
the root, the walk takes one step and then continues in a subtree of
size $#n#_1$ or $#n#_2$.  By our inductive hypothesis, the expected
length of the walk is then
\[
    \E[W] = 1 + \frac{1}{2}\log (#n#_1+1) + \frac{1}{2}\log (#n#_2+1)  \enspace , 
\] 
since each of $#n#_1$ and $#n#_2$ are less than $#n#$.  Since $\log$
is a concave function, $\E[W]$ is maximized when $#n#_1=#n#_2=(#n#-1)/2$.
%To maximize this,
%over all choices of $#n#_1\in[0,#n#-1]$, we take the derivative and obtain
%\[
%    (\E[W])' = \frac{1}{2}(c/#n#_1 - c/(#n#-#n#_1-1)) \enspace , 
%\]
%which is equal to 0 when $#n#_1 = (#n#-1)/2$.  We can establish that
%this is a maximum fairly easily, so
Therefore,
 the expected number of steps taken by the random walk is 
\begin{align*}
    \E[W] 
    & = 1 + \frac{1}{2}\log (#n#_1+1) + \frac{1}{2}\log (#n#_2+1) \\
   & \le  1 + \log ((#n#-1)/2+1) \\
   & =  1 + \log ((#n#+1)/2) \\
   & =  \log (#n#+1)  \enspace . \qedhere 
\end{align*}
\end{proof}

We make a quick digression to note that, for readers who know a little
about information theory, the proof of \lemref{tree-random-walk} can
be stated in terms of entropy.  
\begin{proof}[Information Theoretic Proof of \lemref{tree-random-walk}]
Let $d_i$ denote the depth of the $i$th external node and recall that a
binary tree with #n# nodes has #n+1# external nodes.  The probability
of the random walk reaching the $i$th external node is exactly
$p_i=1/2^{d_i}$, so the expected length of the random walk is given by
\[
   H=\sum_{i=0}^{#n#} p_id_i
    =\sum_{i=0}^{#n#} p_i\log\left(2^{d_i}\right)
    = \sum_{i=0}^{#n#}p_i\log({1/p_i})
\]
The right hand side of this equation is easily recognizable as the
entropy of a probability distribution over $#n#+1$ elements.  A basic
fact about the entropy of a distribution over $#n#+1$ elements is that
it does not exceed $\log(#n#+1)$, which proves the lemma.
\end{proof}

With this result on random walks, we can now easily prove that the
running time of the #merge(h1,h2)# operation is $O(\log #n#)$.

\begin{lem}
  If #h1# and #h2# are the roots of two heaps containing $#n#_1$
  and $#n#_2$ nodes, respectively, then the expected running time of
  #merge(h1,h2)# is at most $O(\log #n#)$, where $#n#=#n#_1+#n#_2$.
\end{lem}

\begin{proof}
  Each step of the merge algorithm takes one step of a random walk,
  either in the heap rooted at #h1# or the heap rooted at #h2#.
  %, depending on whether $#h1.x# < #h2.x#$ or not.
  The algorithm terminates when either of these two random walks fall
  out of its corresponding tree (when $#h1#=#null#$ or $#h2#=#null#$).
  Therefore, the expected number of steps performed by the merge algorithm
  is at most
  \[
     \log (#n#_1+1) + \log (#n#_2+1) \le 2\log #n# \enspace . \qedhere
  \]
\end{proof}

\subsection{Summary}

The following theorem summarizes the performance of a #MeldableHeap#:

\begin{thm}\thmlabel{meldableheap}
  A #MeldableHeap# implements the (priority) #Queue# interface.
  A #MeldableHeap# supports the operations #add(x)# and #remove()#
  in $O(\log #n#)$ expected time per operation.
\end{thm}

\section{Discussion and Exercises}

The implicit representation of a complete binary tree as an array,
or list, seems to have been first proposed by Eytzinger \cite{e1590}.
He used this representation in books containing pedigree family trees
\index{pedigree family tree}%
of noble families.  The #BinaryHeap# data structure described here was
first introduced by Williams \cite{w64}.

The randomized #MeldableHeap# data structure described here appears
to have first been proposed by Gambin and Malinowski \cite{gm98}.
Other meldable heap implementations exist, including 
leftist heaps \cite[Section~5.3.2]{c72,k97v3},
\index{leftist heap}%
\index{heap!leftist}%
binomial heaps \cite{v78},
\index{binomial heap}%
\index{heap!binomial}%
Fibonacci heaps \cite{ft87}, 
\index{Fibonacci heap}%
\index{heap!Fibonacci}%
pairing heaps \cite{fsst86},\
\index{pairing heap}%
\index{heap!pairing}%
 and skew heaps \cite{st83}, 
\index{skew heap}%
\index{heap!skew}%
although none of these are as simple as the #MeldableHeap#
structure.

Some of the above structures also support a #decreaseKey(u,y)# operation
\index{decreaseKey@#decreaseKey(u,y)#}%
in which the value stored at node #u# is decreased to #y#.  (It is a
pre-condition that $#y#\le#u.x#$.)  In most of the preceding structures,
this operation can be supported in $O(\log #n#)$ time by removing node
#u# and adding  #y#.  However, some of these structures can implement
#decreaseKey(u,y)# more efficiently.  In particular, #decreaseKey(u,y)#
takes $O(1)$ amortized time in Fibonacci heaps and $O(\log\log #n#)$
amortized time in a special version of pairing heaps \cite{e09}.
This more efficient #decreaseKey(u,y)# operation has applications in
speeding up several graph algorithms, including Dijkstra's shortest path
algorithm \cite{ft87}.

\begin{exc}
  Illustrate the addition of the values 7 and then 3 to the #BinaryHeap#
  shown at the end of \figref{heap-insert}.
\end{exc}

\begin{exc}
  Illustrate the removal of the next two values (6 and 8) on the
  #BinaryHeap# shown at the end of \figref{heap-remove}.
\end{exc}

\begin{exc}
  Implement the #remove(i)# method, that removes the value stored in
  #a[i]# in a #BinaryHeap#.  This method should run in $O(\log #n#)$ time.
  Next, explain why this method is not likely to be useful.
\end{exc}

\begin{exc}\exclabel{general-eytzinger}
  \index{tree!$d$-ary}%
  A $d$-ary tree is a generalization of a binary tree in which each
  internal node has $d$ children.  Using Eytzinger's method it is also
  possible to represent complete $d$-ary trees using arrays.  Work out
  the equations that, given an index #i#, determine the index of #i#'s
  parent and each of #i#'s $d$ children in this representation.
\end{exc}

\begin{exc}
  \index{DaryHeap@#DaryHeap#}%
  Using what you learned in \excref{general-eytzinger}, design and
  implement a \emph{#DaryHeap#}, the $d$-ary generalization of a
  #BinaryHeap#. Analyze the running times of operations on a #DaryHeap#
  and test the performance of your #DaryHeap# implementation against
  that of the #BinaryHeap# implementation given here.
\end{exc}



\begin{exc}
  Illustrate the addition of the values 17 and then 82 in the
  #MeldableHeap# #h1# shown in \figref{meldable-merge}.  Use a coin to
  simulate a random bit when needed.
\end{exc}

\begin{exc}
  Illustrate the removal of the next two values (4 and 8) in the
  #MeldableHeap# #h1# shown in \figref{meldable-merge}.  Use a coin to
  simulate a random bit when needed.
\end{exc}

\begin{exc}
  Implement the #remove(u)# method, that removes the node #u# from
  a #MeldableHeap#.  This method should run in $O(\log #n#)$ expected time.
\end{exc}

\begin{exc}
  Show how to find the second smallest value in a #BinaryHeap# or
  #MeldableHeap# in constant time.
\end{exc}

\begin{exc}
  Show how to find the $k$th smallest value in a #BinaryHeap# or
  #MeldableHeap# in $O(k\log k)$ time.  (Hint: Using another heap
  might help.)
\end{exc}

\begin{exc}
  Suppose you are given #k# sorted lists, of total length #n#.  Using
  a heap, show how to merge these into a single sorted list in $O(n\log
  k)$ time.  (Hint: Starting with the case $k=2$ can be instructive.)
\end{exc}








