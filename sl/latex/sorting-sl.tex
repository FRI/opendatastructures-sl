\chapter{Algoritmi za urejanje}
\chaplabel{sorting}
\translatedby{Matic Teršek}{sl}
\translatedby{Nejc Thaler}{sl}
\translatedby{Aleš Turk}{sl}

V tem poglavju se bomo pogovarjali o algoritmih, ki uredijo zbirko #n# elementov. To se lahko sliši kot čudna tema v knjigi podatkovnih struktur, ampak za to obstaja nekaj dobrih razlogov. Najočitnejši je ta, da sta dva od urejevalnih algoritmov (hitro urejanje in urejanje s kopico) tesno povezana s podatkovnima strukturama, ki smo ju že obdelali (naključno binarno drevo in kopice).

V prvem delu tega poglavja bo govora o algoritmih, ki uporabljajo zgolj primerjanje in sicer tri take algoritme s časovno zahtevnostjo $O(#n#\log #n#)$.
Kot se izkaže, so vsi trije algoritmi asimptotično optimalni. Se pravi vsak algoritem, ki uporablja zgolj primerjanje izvede približno $#n#\log#n#$ primerjav v najslabšem kot tudi v povprečnem primeru.

Preden nadaljujemo velja izpostaviti, da lahko uporabimo katerikoli implementacijo urejene množice ali prioritetne vrste, ki smo jih predstavili v prejšnjih poglavjih, da dobimo algoritem za urejanje s časovno zahtevnostjo $O(#n#\log #n#)$. Naprimer, lahko uredimo #n# elemente tako, da izvedemo najprej #n# #add(x)# operacij, nato pa #n# #remove()# operacij na binarni ali zdržljivi kopici. Alternativno lahko uporabimo tudi #n# #add(x)# operacij na katerimkoli binarnim iskalnim drevesom, kjer nato izvedemo vmesno prečkanje (vaja \excref{tree-traversal}), da dobimo elemente v urejenem vrstnem redu. Vendar si v obeh primerih naredimo veliko preglavic, da zgradimo strukturo, ki je nikoli ne uporabljamo v celoti. Urejanje je tako pomembna težava, da je vredno razviti direktne metode, ki so kot se le da hitre, preproste in prostorsko učnkovite.

Drugi del tega poglavja kaže, da ne obstaja časovnih zagotovil, če uporabimo katerekoli druge operacije razen primerjave. Je pa res, da lahko s tabelnim indeksiranjem uredimo množico #n# števil v območju $\{0,\ldots,#n#^c-1\}$ s časovno zahtevnostjo $O(c#n#)$.

\section{Urenanje s primerjanjem}
\index{comparison-based sorting}%
\index{sorting algorithm!comparison-based}%
V tem delu bomo predstavili tri algoritme za urejanje: urejanje z zlivanjem, hitro urejanje in urejanje s kopico. Vsak izmed teh treh algoritmov sprejme kot prvi argument tabelo #a#, ki jo uredi v naraščujočem vrstnem redu v (pričakovanem) času $O(#n#\log #n#)$. Vsi ti algoritmi delujejo \emph{na osnovi primerjav}. Njihov drugi argument #c# je #Comparator# ki implementira metodo #compare(a,b)#. Ti algoritmi za urejanje nimajo privzetega tipa podatkov, ker izvajajo zgolj operacijo compare(a,b). Spomnimo se poglavja \secref{sset}, kjer smo se naučili, da #compare(a,b)# vrača negativno vrednost če je $#a#<#b#$, pozitivno, če je vrednost $#a#>#b#$ in nič, če je $#a#=#b#$.

\subsection{Urejanje z zlivanjem (merge-sort)}
\seclabel{merge-sort}

\index{merge-sort}%
Algoritem \emph{urejanja z zlivanjem} je klasičen primer rekurzivnega algoritma deli in vladaj.
Če je dolžina od #a# največ 1, potem je #a# že urejen in zato ne naredimo ničesar. V nasprotnem primeru
pa #a# razdelimo na dva dela, $#a0#=#a#[0],\ldots,#a#[#n#/2-1]$ in $#a1#=#a#[#n#/2],\ldots,#a#[#n#-1]$. Nato rekurzivno uredimo #a0# in #a1# ter ju nato združimo s čimer dobimo popolno urejeno tabelo #a#.
\javaimport{ods/Algorithms.mergeSort(a,c)}
\cppimport{ods/Algorithms.mergeSort(a)}
Primer \figref{merge-sort}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort}
  \end{center}
  \caption[Merge sort]{Izvedba #mergeSort(a,c)#}
  \figlabel{merge-sort}
\end{figure}

V primerjavi z urejanjem je zlivanje urejenih tabel #a0# in #a1# dokaj enostavno. Elemente dodajamo enega za drugim. Če je #a0# ali #a1# prazna, potem dodajamo naslednje elementi iz druge (ne prazne) tabele. Sicer vzamemo manjšega od nasledjih elementov iz obeh tabel in ga dodamo v #a#:

\javaimport{ods/Algorithms.merge(a0,a1,a,c)}
\cppimport{ods/Algorithms.merge(a0,a1,a)}

Opazimo, da algoritem #merge(a0,a1,a,c)# v najslabšem primeru izvede $#n#-1$ primerjav, preden izprazne #a0# ali #a1#.

Da bi lažje razumeli čas izvajanja urejanja z zlivanjem, si ga je najbolje predstavljati kot njegovo rekurzivno drevo. Zaenkrat predpostavimo, da je #n# potenca števila dve, tako da je $#n#=2^{\log #n#}$, $\log #n#$ pa je celo število.
Glej sliko \figref{mergesort-recursion}. Urejanje z zlivanjem spremeni problem urejanja #n# elementov v dva problema urejanja $#n#/2$ elementov. Ta dva podproblema potem spremeni vsakega v dva nova podproblema, torej skupno v štiri probleme velikosti $#n#/4$. Te štiri nato razdelimo v osem podproblemov velikosti $#n#/8$ in tako dalje. Na koncu tega procesa $#n#/2$ podproblemov, vsakega velikosti dve, razdelimo v #n# problemov velikosti ena. Za vsak podproblem velikosti $#n#/2^{i}$ je čas zlivanja in kopiranja podatkov razreda $O(#n#/2^i)$. Ker imamo $2^i$ podproblemov velikosti $#n#/2^i$, je skupen čas reševanja problemov velikosti $2^i$, če ne štejemo rekurzivnih klicev:
$$2^i\times O(n/2^i) = O(n) \enspace .$$
Iz tega sledi, da je skupen čas, ki ga porabi urejanje z zlivanjem:
$$\sum_{i=0}^{\log n} O(n) = O(n\log n) \enspace .$$

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/mergesort-recursion}
    \caption{The merge-sort recursion tree.}
    \figlabel{mergesort-recursion}
  \end{center}
\end{figure}


Dokaz za naslednji izrek je osnovan na prejšnji analizi, vendar pa moramo biti pazljivi zaradi primera, kadar #n# ni potenca števila 2.
\begin{thm}
  Algoritem #mergeSort(a,c)# teče v času $O(#n#\log #n#)$ in izvede največ $#n#\log #n#$ primerjav.
\end{thm}

\begin{proof}
Dokažemo z indukcijo po $#n#$. Osnovni primer, kadar je $#n#=1$, je trivialen; če algoritem v obdelavo dobi tabelo dolžine 0 ali 1, to tabelo enostavno vrne, brez da bi izvedel kakršne koli primerjave.

Zlivanje dveh urejenih seznamov skupne dolžine $#n#$ zahteva največ $#n#-1$ primerjav. Naj $C(#n#)$ označuje največje možno število primerjav, ki jih izvede #mergeSort(a,c)# na tabeli #a# dolžine #n#. Če je #n# sodo število, potem za podproblema uporabimo indukcijsko hipotezo in s tem dobimo:
\begin{align*}
  C(#n#) 
  &\le #n#-1 + 2C(#n#/2) \\
  &\le #n#-1 + 2((#n#/2)\log(#n#/2)) \\
  &= #n#-1 + #n#\log(#n#/2) \\
  &= #n#-1 + #n#\log #n#-#n# \\
  &< #n#\log #n# \enspace .
\end{align*}
Če je #n# liho število pa je dokaz nekoliko bolj zapleten. V tem primeru uporabimo dve neenačbi, ki jih lahko enostavno dokažemo:
\begin{equation}
  \log(x+1) \le \log(x) + 1 \enspace , \eqlabel{log-ineq-a}
\end{equation}
za vse $x\ge 1$ in:
\begin{equation}
  \log(x+1/2) + \log(x-1/2) \le 2\log(x) \enspace , \eqlabel{log-ineq-b}
\end{equation}
za vse $x\ge 1/2$. Neenačbo~\myeqref{log-ineq-a} izpeljemo iz dejstva, da je $\log(x)+1 = \log(2x)$, \myeqref{log-ineq-b} pa izpeljemo iz dejstva, da je $\log$ konkavna funkcija.
Ko vemo vse to, za lihi #n# velja:
\begin{align*}
  C(#n#) 
  &\le #n#-1 + C(\lceil #n#/2 \rceil) + C(\lfloor #n#/2 \rfloor) \\
  &\le #n#-1 + \lceil #n#/2 \rceil\log \lceil #n#/2 \rceil 
           + \lfloor #n#/2 \rfloor\log \lfloor #n#/2 \rfloor \\
  &= #n#-1 + (#n#/2 + 1/2)\log (#n#/2+1/2) 
           + (#n#/2 - 1/2) \log (#n#/2-1/2) \\
  &\le #n#-1 + #n#\log(#n#/2) + (1/2)(\log (#n#/2+1/2) 
           - \log (#n#/2-1/2)) \\
  &\le #n#-1 + #n#\log(#n#/2) + 1/2 \\
  &< #n# + #n#\log(#n#/2) \\
  &= #n# + #n#(\log#n#-1) \\
  &= #n#\log#n# \enspace . \qedhere
\end{align*} 
\end{proof}

\subsection{Hitro urejanje (quicksort)}
\translatedby{Domen Soklič}{sl}
\translatedby{Rok Razinger}{sl}
\translatedby{Rok Samsa}{sl}

\index{quicksort}%
Hitro urejanje ali \emph{quicksort} algoritem je še en klasični algoritem deli in vladaj . V nasprotju z algoritmom zlivanja (mergesort), ki združuje rešitvi dveh podproblemov, algoritem hitrega urejanja počne vse svoje delo vnaprej.\\

Algoritem lahko preprosto opišemo tako: Izberemo naključni delilni element, ki ga imenujemo \emph{pivot}, \index{pivot element}%
#x#, ki ga dobimo iz #a#; Razdelek #a# je sestavljena iz sklopa elementov manjših od #x#, sklopa elementov enakih kot #x# in niz elementov večjih od #x#; na koncu rekurzivno razvrstimo prvi in tretji sklop števil v tem razdelku. Primer je prikazan na sliki \figref{quicksort}.
\javaimport{ods/Algorithms.quickSort(a,c).quickSort(a,i,n,c)}
\cppimport{ods/Algorithms.quickSort(a).quickSort(a,i,n)}
\begin{figure}[h]
  \begin{center}
    \includegraphics[scale=0.90909]{figs/quicksort}
    \caption[Quicksort]{Primer izvajanja \javaonly{#quickSort(a,0,14,c)#} \cpponly{#quickSort(a,0,14)#}}
    \figlabel{quicksort}
  \end{center}
\end{figure}
Vse to je narejeno na mestu, tako da namesto ustvarjanja kopij urejenih podseznamov, #quickSort(a,i,n,c)# metoda razvršča samo podseznam $#a[i]#,\ldots,#a[i+n-1]#$. Prvotno kličemo to metodo kot\\
#quickSort(a,0,a.length,c)#. \\

V središču algoritma quicksort je algoritem delitve na mestu. Ta algoritem, brez uporabe dodatnega prostora, zamenja elemente v #a# in izračuna indekse #p# in #q# tako da:
\[
   #a[i]# \begin{cases} 
         {}< #x# & \text{če $0\le #i#\le #p#$} \\
         {}= #x# & \text{če $#p#< #i# < #q#$} \\
         {}> #x# & \text{če $#q#\le #i# \le #n#-1$}
     \end{cases}
\]

Ta delitev, ki se opravi z #while# zanko v sami kodi, deluje s ponavljajočim povečanjem #p#-ja in zmanjševanjem #q#-ja ob ohranjanju prvega in zadnjega od teh pogojev (#p# in #q#). Ob vsakem koraku element na položaju #j# premaknemo na prvo mesto, ali pa na zadnje mesto. V prvih dveh primerih, je #j# povečan, v zadnjem primeru pa ne, ker nov element na položaju #j# še ni bil obdelan.\\

Quicksort algoritem je zelo tesno povezan z naključnim dvojiškim iskalnim drevesom, opisanem v poglavju \secref{rbst}. Pravzaprav, če poženemo quicksort algoritem nad #n# različnimi elementi, potem je quicksort-ovo rekurzivno drevo enako naključnemu iskalnemu drevesu. Da bi to videli, se moramo spomniti, kako gradimo naključno dvojiška iskalna drevesa. Najprej naključno izberemo element #x# in ga postavimo za koren drevesa. Nato vsak naslednji element primerjamo z #x#-om. Manjše elemente postavljamo v levo poddrevo, večje pa v desno poddrevo.\\

S tem algoritmom izberemo nakjučni element #x# in takoj za tem primerjamo vse elemente s tem #x#-om. Najmanjše elemente postavimo na začetek polja, večje pa postavimo na konec. Quicksort algoritem nato rekurzivno uredi začetek in konec polja, medtem ko naključno dvojiško iskalno drevo rekurzivno vstavi manjše elemente v levo poddrevo korena in večje elemente v desno poddrevo korena.\\

Zgornje ujemanje med naključnim dvojiškim iskalnim drevesom in algoritmom hitrega urejanja lahko uporabimo za lemo \lemref{rbs}

\begin{lem}\lemlabel{quicksort}
Ko kličemo algoritem quicksort za urejanje polja, ki vsebuje cela števila $0,\ldots,#n#-1$, je pričakovano število primerjav elementa s pivot-om $H_{#i#+1} + H_{#n#-#i#}$.
\end{lem}

Malo seštevanja harmoničnih števil nam da naslednji izrek o času delovanja, katerega porabi algoritem:

\begin{thm}\thmlabel{quicksort-i}
Ko quicksort algoritem uporabimo za urejanje polja z #n# različnimi elementi, pričakujemo največje število opravljenih primerjav $2#n#\ln #n# + O(#n#)$.
\end{thm}

\begin{proof}
Naj bo $T$ število primerjav opravljenih z algoritmom quicksort, ko razvršča $n$ različnih elementov. Z uporabo Leme \lemref{quicksort}, imamo:
\begin{align*}
  \E[T] &= \sum_{i=0}^{#n#-1}(H_{#i#+1}+H_{#n#-#i#}) \\ 
        &= 2\sum_{i=1}^{#n#}H_i \\ 
        &\le 2\sum_{i=1}^{#n#}H_{#n#} \\ 
        &\le 2#n#\ln#n# + 2#n# = 2#n#\ln #n# + O(#n#) \qedhere
\end{align*}
\end{proof}

Izrek \thmref{quicksort} opisuje primer, kjer so razvrščeni elementi vsi različni. Ko vhodni seznam #a#, vsebuje podvojene elemente, pričakovani čas delovanja za hitro urejanje ni nič slabši, in je lahko celo boljši. Vedno ko je podvojeni element #x# izbran kot pivot #a#, vse pojavitve #x#-a združimo in jih kasneje ne vključimo v enega od dveh podproblemov.

\begin{thm}\thmlabel{quicksort}
Časovna zahtevnost metode #Quicksort(a, c)# je $O(#n#\log #n#)$, pričakovano število primerjav, ki jih opravi, je večinoma $2#n#\ln #n# +O(#n#)$.
\end{thm}

\subsection{Urejanje s kopico (heap-sort)}
\seclabel{heapsort}

\index{heap-sort}%
Algoritem \emph{Heap-sort} je še eden izmed algoritmov urejanja na mestu. Heap-sort uporablja dvojiško kopico, ki smo jo obravnavali v poglavju \secref{binaryheap}. Spomnimo se, da podatkovna struktura #BinaryHeap#" predstavlja kopico, ki je realizirana z enim seznamom. Urejanje s kopico pretvori vhodni seznam #a# v kopico in nato ponavljajoče izloča minimalno vrednost.\\

Bolj natančno povedano, kopica hrani #n# elementov v seznamu #a# na lokacijah $#a[0]#,\ldots,#a[n-1]#$ z najmanjšo vrednostjo v korenu oz. #a[0]#. Po transformaciji v #BinaryHeap# urejanje s kopico ponavljajoče izmenjuje #a[0]# in #a[n-1]#, ter kliče #trickleDown(0)#, tako da so $#a[0]#,\ldots,#a[n-2]#$ ponovno v kopičasti ureditvi. Ko se ta proces konča (ker je $#n#=0$), so elementi #a# shranjeni v padajočem zaporedju. Sedaj #a# obrnemo, da dobimo rezultat. \footnote{Algoritem bi lahko prav tako redefiniral #compare(x,y)#, tako da algoritem že na začetku vstavi elemente v naraščajočem vrstnem redu.}
Na sliki \figref{heapsort} je primer izvajanja #heapSort(a,c)#.
\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/heapsort}
  \end{center}
  \caption[Heap sort]{Primer izvedbe #heapSort(a,c)#.}
  \figlabel{heapsort}
\end{figure}

\javaimport{ods/BinaryHeap.sort(a,c)}
\cppimport{ods/BinaryHeap.sort(b)}

Ključna podrutina v heap-sort je konstruktor za pretvorbo urejenega seznama #a# v kopico. To bi z lahkoto storili v času $O(#n#\log#n#)$ s ponavljajočim klicem metode #add(x)# dvojiške kopice, a znamo to operacijo izvesti hitreje z uporabo bottom-up algoritma.
Spomnimo se, da so v binarni kopici otroci #a[i]# shranjeni na položajih #a[2i+1]# in #a[2i+2]#. To namiguje, da elementi $#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$ nimajo otrok. Z drugimi besedami je vsak element $#a#[\lfloor#n#/2\rfloor],\ldots,#a[n-1]#$
podkopica velikosti 1. Sedaj ko delamo od zadaj naprej, lahko kličemo metodo #trickleDown(i)# za vsak $#i#\in\{\lfloor #n#/2\rfloor-1,\ldots,0\}$. To deluje, ker je do trenutka ko kličemo #trickleDown(i)# vsak od otrok #a[i]# koren podkopice. 
S tem ko kličemo #trickleDown(i)#, nastavimo #a[i]# kot koren svoje podkopice.
\javaimport{ods/BinaryHeap.BinaryHeap(a,c)}
\cppimport{ods/BinaryHeap.BinaryHeap(b)}

Zanimivost te bottom-up strategije je, da je bolj učinkovita kot klicanje metode #add(x)# #n#-krat. Opazimo, da za $#n#/2$ elementov sploh ne delamo, za $#n#/4$ elementov kličemo #trickleDown(i)# nad podkopico, katere koren je $a[i]$ in je njena višina enaka 1.
Za $#n#/8$ elementov kličemo metodo #trickleDown(i)# nad podkopici katere višina je enaka 2 in tako dalje.
Ker je delo, ki ga izvaja #trickleDown(i)# sorazmerno višini podkopice #a[i]#, je celotnega dela največ
\[
    \sum_{i=1}^{\log#n#} O((i-1)#n#/2^{i})
    \le \sum_{i=1}^{\infty} O(i#n#/2^{i})
    = O(#n#)\sum_{i=1}^{\infty} i/2^{i}
    =  O(2#n#) = O(#n#) \enspace .
\]
Predzadnja enakost sledi, ker je seštevek
$\sum_{i=1}^{\infty} i/2^{i}$ po definiciji enak pričakovanemu številu glav ob metu kovanc ob uporabi leme \lemref{met-kovanca}.

Naslednji izrek opisuje zmogljivost metode #heapSort(a,c)#.
\begin{thm}
  Metoda #heapSort(a,c)# se izvede v času $O(#n#\log #n#)$ in izvede največ $2#n#\log #n# + O(#n#)$ primerjav.
\end{thm}

\begin{proof}
Algoritem deluje v treh korakih:  (1)~Pretvorba #a# v kopico,
(2)~ponavljajoče izločanje najmanjšega elementa iz #a# in (3)~obrne elemente v #a#.  
Ravno smo zatrdili da korak~1 potrebuje $O(#n#)$
časa za izvedbo in $O(#n#)$ primerjav. Korak~3  potrebuje $O(#n#)$ čaza za izvedbo in nič primerjav.  
Korak~2 izvede #n# klicev metode #trickleDown(0)#.
$i$-ti klic se izvaja na kopici velikosti $#n#-i$ in izvede največ 
$2\log(#n#-i)$ primerjav. Če seštejemo preko #i# dobimo
\[
   \sum_{i=0}^{#n#-i} 2\log(#n#-i) 
   \le \sum_{i=0}^{#n#-i} 2\log #n#
   =  2#n#\log #n#
\]
S tem ko dodamo število izvedenih primerjav v vsakem od treh korakov dokončamo dokaz.
\end{proof}


\subsection{Spodnja meja algoritmov za urejanje, temelječih na primerjavah}

\translatedby{Gašper Pustinek}{sl}
\translatedby{Robert Finžgar}{sl}

\index{lower-bound}%
\index{sorting lower-bound}%
Sedaj smo videli tri primerjalne algoritme za urejanje, ki imajo časovno zahtevnost $O(#n#\log #n#)$.
Čas je da se vprašamo, če obstaja hitrejši algoritem. Kratek odgovor, je ne. Če je edina
dovoljena operacija primerjava dveh elementov #a#, potem ni algoritma, ki se lahko izogne približno
$#n#\log #n#$ primerjavam. To ni težko dokazati in izhaja iz
\[
   \log(#n#!) 
     = \log #n# + \log (#n#-1) + \dots + \log(1) 
     = #n#\log #n# - O(#n#)
    \enspace .
\]
\noindent
(Dokaz te formule je \excref{log-factorial}.)

Najprej bomo pozornost namenili determinističnim algoritmom, kot sta razvrščanje z zlivanjem in
urejanje s kopico. Predstavljajte si, da tak algoritem uporabimo za urejanje #n# različnih elementov.

Pri dokazovanju spodnje meje je ključno opažanje, da je za deterministične algoritme z enako vrednostjo
#n#, prva primerjava vedno enaka. Na primer pri #heapSort(a,c)#, ko je #n# liho, je prvi klic #trickleDown(i)#
 s vrednostjo #i=n/2-1#, in prva primerjava med elementoma #a[n/2-1]# in #a[n-1]#.
 
Ker se elementi ne ponavljajo, ima prva primerjava samo dva možna izida. Druga primerjava pa je
 lahko odvisna od prve. Tretja je pa lahko odvisna od prve in druge in tako naprej. Na ta način, si 
 lahko kateri koli deterministični primerjalni algoritem za urejanje predstavljamo kot \emph{comparison tree} s korenom.
 Vsako notranje vozlišče #u#, tega drevesa, je označeno s parom indeksov
 #u.i# in #u.j#. Če je $#a[u.i]#<#a[u.j]#$, algoritem nadaljuje pot po levem pod drevesu, drugače pa po
 desnem pod drevesu. Vsak list #w#, je označen s permutacijo $#w.p[0]#,\ldots,#w.p[n-1]#$ pri $0,\ldots,#n#-1$. To
 permutacijo potrebujemo za urejanje polja #a#, če primerjalno drevo obišče ta list. To je,
\[
   #a[w.p[0]]#<#a[w.p[1]]#<\cdots<#a[w.p[n-1]]# \enspace .
\]
 Primer primerjalnega drevesa za polje dolžine #n=3#, je prikazano na sliki
\figref{comparison-tree}.
\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree}
  \end{center}
  \caption[A comparison tree]{Primerjalno drevo za urejanje polja $#a[0]#,#a[1]#,#a[2]#$ dolžine #n=3#.}
  \figlabel{comparison-tree}
\end{figure}


Primerjalno drevo za algoritem za urejanje, nam pove vse o njem. Pove nam natančno zaporedje primerjav,
 ki jih bo algoritem izvedel na nekem polju #a#, ki ima #n# različnih elementov, in nam pove, kako bo
 algoritem spremenil vrstni red polja #a#, da ga bo uredil. Posledično, mora primerjalno drevo vsebovati
 vsaj $#n#!$ listov. Če nima, pomeni da obstajata 2 različni permutaciji, ki vodita do istega lista,
 zato, algoritem ne uredi pravilno vsaj ene od permutacij.
 
 Na primer, primerjalno drevo prikazano na \figref{comparison-tree-2} ima samo $4< 3!=6$ listov. Če pregledamo drevo, opazimo,
da za polji z elementi $3,1,2$ in $3,2,1$ oba vodita do istega lista. Za polje $3,1,2$ dobimo pravilen izhod
$#a[1]#=1,#a[2]#=2,#a[0]#=3$. Ampak če imamo na vhodu $3,2,1$, nas napačno vodi do $#a[1]#=2,#a[2]#=1,#a[0]#=3$. To vodi
do osnovne spodnje meje za urejevalne algoritme, ki temeljijo na primerjavah.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/comparison-tree-b}
  \end{center}
  \caption{Primerjalno drevo, ki ne uredi pravilno vseh možnih vhodnih podatkov.}
  \figlabel{comparison-tree-2}
\end{figure}

\begin{thm}\thmlabel{deterministic-sorting-lower-bound}
 Za kateri koli deterministični algoritem za urejanje $\mathcal{A}$ , ki temelji na primerjavah, in
 za katero koli celo število $#n#\ge 1$, obstaja tako vhodno polje #a#
 dolžine #n#, da se izvede vsaj $\log(#n#!) =
 #n#\log#n#-O(#n#)$ primerjav, ko urejamo #a#.
\end{thm}

\begin{proof}
 Primerjalno drevo definirano kot $\mathcal{A}$, mora imeti vsaj $#n#!$ listov. Preprost dokaz z indukcijo
nam pokaže, da ima vsako dvojiško drevo s $k$ listi, višino vsaj $\log k$. Posledično mora imeti primerjalno
drevo $\mathcal{A}$ list #w#, ki ima globino vsaj $\log(#n#!)$ in obstaja tako vhodno polje #a#, da vodi do tega lista.
Polje #a# je tako, da $\mathcal{A}$ izvede vsaj $\log(#n#!)$ primerjav.
\end{proof}

Izrek \thmref{deterministic-sorting-lower-bound} govori o determinističnih algoritmih, kot sta razvrščanje z zlivanjem in urejanje s kopico.
Kaj pa če imamo naključen algoritem kot je hitro urejanje? Ali bi lahko naključen algoritem bil
boljši od spodnje meje $\log(#n#!)$ primerjav? Odgovor, je ponovno, ne. To lahko dokažemo, če na
to, kaj je naključen algoritem, pomislimo malo drugače.

Predvidevali bomo, da je naše odločitveno drevo ``očiščeno'': Vsako vozlišče, ki ga ne moremo obiskati
z nekim vhodnim poljem #a#, odrežemo. To pomeni, da bo imelo drevo natanko $#n#!$ listov. Ima vsaj $#n#!$
listov, ker drugače ne bi uredilo polje pravilno. Ima največ $#n#!$ listov, ker za vsako od $#n#!$ permutacij 
#n# elementov, obstaja natanko en koren, ki vodi do tega lista.

Na algoritem za urejanje $\mathcal{R}$, ki ima naključnost, lahko gledamo kot deterministični algoritem, ki sprejme
2 vhoda: Polje #a#, ki ga bomo uredili, in dolgo zaporedje $b=b_1,b_2,b_3,\ldots,b_m$ naključnih realnih števil
v obsegu $[0,1]$. Naključna števila potrebujemo za naključnost v algoritmu. Ko želi met kovanca ali
naključno odločitev, uporabi eno od vrednosti iz $b$. Na primer, če želimo izračunati indeks prvega
pivota pri hitrem urejanju, lahko algoritem uporabi formulo $\lfloor n b_1\rfloor$.

Če za $b$ uporabimo neko določeno zaporedje $\hat{b}$, potem $\mathcal{R}$ postane deterministični algoritem za urejanje,
$\mathcal{R}(\hat{b})$, ki ima primerjalno drevo $\mathcal{T}(\hat{b})$. Če za #a# izberemo naključno permutacijo iz $\{1,\ldots,#n#\}$,
potem je to ekvivalentno izbiri naključnega lista #w#, od $#n#!$ listov od $\mathcal{T}(\hat{b})$.

Naloga \excref{randomized-lower-bound} zahteva dokaz, da če izberemo naključen list iz dvojiškega drevesa, ki ima $k$ listov, potem
je pričakovana globina tega lista vsaj $\log k$. Zaradi tega je pričakovana vrednost primerjav
(determinističnega) algoritma $\mathcal{R}(\hat{b})$, ki sprejme za vhod naključno permutacijo iz $\{1,\ldots,n\}$, vsaj
$\log(#n#!)$. To velja za vsako izbiro $\hat{b}$, zato to velja tudi za $\mathcal{R}$. To zaključi dokaz o spodnji meji
za naključni algoritem.

\begin{thm}\thmlabel{randomized-sorting-lower-bound}
 Za vsako celo število $n\ge 1$ in kateri koli (deterministični ali naključni) primerjalni
 algoritem za urejanje $\mathcal{A}$, je pričakovana vrednost primerjav, ki jih stori algoritem pri naključni permutaciji
 $\{1,\ldots,n\}$, vsaj $\log(#n#!) = #n#\log#n#-O(#n#)$.
\end{thm}

\section{Counting Sort and Radix Sort}

V tem delu preučujemo dva urejevalna algoritma ki nista bazirana na primerjanju.
Algoritma sta specializirana za ločevanje manjših celih števil, ter se izogneta spodnji
meji izreka \thmref{deterministic-sorting-lower-bound}
z uporabo (nekaterih delov) elementov v #a# kot indeksov v polju.
Razmislite o izrazu
\[
  #c[a[i]]# = 1 \enspace .
\]
Ta izraz se izvrši v konstantnem času , ampak ima #c.length# možnih
različnih rezultatov, odvisno od vrednosti #a[i]#.  To pomeni da izvršitev
algoritma ki poda tako izjavo ni mogoče modelirati kot binarno drevo.
To je glavni razlog da so algoritmi v tem delu zmožni urejati hitreje kot
algoritmi bazirani na primerjavah.

\subsection{Counting Sort}

Recimo da imamo polje #a# sestavljeno iz #n# celih števil, vse v
obsegu $0,\ldots,#k#-1$. Algoritm \emph{urejanje s štetjem}
\index{counting-sort}%
urejanja #a# z uporabo pomožnega polja 
števcev #c#. Ven dobimo urejeno verzijo
polja #a# kot pomožno polje #b#.

Ideja pri urejanju s štetjem je preprosta:  Za vsak
$#i#\in\{0,\ldots,#k#-1\}$, prešteje število
pojavitev #i# v #a# in to shrani v #c[i]#. Po urejanju izhodni
produkt zgleda #c[0]# ponovitev 0, sledi #c[1]# pojavitev 1, sledi
še #c[2]# pojavitev 2,\ldots, sledi #c[k-1]# pojavitev #k-1#.
Koda ki to izvrši je zelo spretna, njeno delovanje je ilustrirano na sliki
\figref{countingsort}:
\codeimport{ods/Algorithms.countingSort(a,k)}

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/countingsort}
  \end{center}
  \caption{Operacija urejanja s štetjem na polju velikosti $#n#=20$, ki shrani $0,\ldots,#k#-1=9$ števil.}
  \figlabel{countingsort}
\end{figure}

Prva #for# zanka v tej kodi nastavi vsak števec #c[i]# tako,
da šteje število ponovitev #i# v #a#.
Z uporabo vrednosti #a# kot indeks se ti števci lahko vsi izračunajo v času $O(#n#)$ z eno samo
for zanko. Na tej točki bi lahko uporabili #c# da direktno zapolnimo
izhodni polje #b#. Vendar pa to ne bi delovalo če bi imeli elementi
polja #a# povezane podatke. Zato porabimo nekaj več
napora da prekopiramo elemente polja #a# v #b#.

Naslednja #for# zanka, ki potrebuje $O(#k#)$ časa, izračuna tekoče
vsote števcev tako da #c[i]# postane število elementov v #a#,
ki so manjši ali enaki #i#. Še posebej
za vsak $#i#\in\{0,\ldots,#k#-1\}$, bo izhodno polje #b# imelo
\[
   #b[c[i-1]]#=#b[c[i-1]+1]=#\cdots=#b[c[i]-1]#=#i# \enspace .
\]
Na koncu algoritem pregleda #a# vzvratno tako, da postavi svoje
elemente v pravem vrstnem redu v izhodno polje #b#. Ko pregleduje, postavi
element #a[i]=j# na pozicijo #b[c[j]-1]# in vrednost #c[j]# se zmanjša.

\begin{thm}
  Metoda #countingSort(a,k)# lahko uredi polje #a#, ki vsebuje #n#
  števil v množici $\{0,\ldots,#k#-1\}$ v času $O(#n#+#k#)$.
\end{thm}

Algoritem urejanje s štetjem  ima prijetno lastnost in sicer da je \emph{stabilen}
\index{stable sorting algorithm}%, ohrani 
relativni vrstni red enakih elementov. Če imata dva elementa
#a[i]#in #a[j]# isto vrednost, in $#i#<#j#$, potem se bo #a[i]# pojavil pred #a[j]# v #b#.
To bo uporabno v naslednjem poglavju.

\subsection{Radix-Sort}

Urejanje s štetjem je zelo efektivna metoda za urejanje polja števil ,
ko je dolžina polja #n# ni veliko manjša kot maksimalna vrednost $#k#-1$,
ki se pojavi v polju. Algoritem \emph{korensko urejanje}
\index{radix-sort}%
, ki ga sedaj opisujemo uporablja 
več prehodov algoritma urejanja s štetjem, kar dopušča večji
razpon maksimalnih vrednosti.

Korensko urejanje ureja #w#-bitna števila z uporabo $#w#/#d#$ prehodov
urejanja s štetjem, da ta števila uredi po #d# bitih naenkrat. \footnote{Privzamemo da
#d# deli #w#, v nasprotnem primeru lahko #w# povečamo na $#d#\lceil
#w#/#d#\rceil$.} Natančneje, korensko urejanje prvo uredi števila po najmanj pomembnih #d# bitih nato po naslednjih #d#
pomembnejših bitih in tako naprej,
v zadnjem prehodu so števila urejena po najpomembnejših #d# bitih.
\codeimport{ods/Algorithms.radixSort(a)}
(V tej kodi, izraz #(a[i] >> d*p)&((1<<d)-1)# izloči število katerega dvojiška predstavitev je dana z biti
$(#p#+1)#d#-1,\ldots,#p##d#$ od #a[i]#.)
Primer korakov tega algoritma je prikazan na skiki \figref{radixsort}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/radixsort}
  \end{center}
  \caption{Uporaba korenskega urejanja za urejanje $#w#=8$-bitnega števila z uporabo štirih prehodov z uporabo urejanja s štetjem na $#d#=2$-bitnih številih}
  \figlabel{radixsort}
\end{figure}

Ta neverjetni algoritem ureja pravilno, ker je urejanje s štetjem
stabilen algoritem za urejanje. Če sta $#x# < #y#$ dva elementa polja
#a# in če ima najpomembnejši bit pri katerem se #x# razlikuje od #y#
index $r$, potem bo #x# postavljen pred #y# med prehodom $\lfloor r/#d#\rfloor$
in vsi naslednji prehodi ne bodo spremenili
relativnega vrstnega reda #x# in #y#.

Korensko urejene opravi #w/d# prehodov urejanja s štetjem. Vsak prehod porabi
$O(#n#+2^{#d#})$ časa. Torej, zahtevnost korenskega urejanja je
izražena v naslednjem izreku.
\begin{thm}\thmlabel{radix-sort}
  Za katerokoli število $#d#>0$, #radixSort(a,k)# metoda lahko uredi polje
  #a#, ki vsebuje #n# #w#-bitnih števil v času $O((#w#/#d#)(#n#+2^{#d#}))$.
\end{thm}

Če namesto elementov polja v razponu
$\{0,\ldots,#n#^c-1\}$ vzamemo $#d#=\lceil\log#n#\rceil$
dobimo nasledno različico izreka \thmref{radix-sort}.
\begin{cor}\corlabel{radix-sort-poly}
  Metoda #radixSort(a,k)# lahko uredi polje #a#, ki vsebuje #n#
  številskih vrednosti v razponu $\{0,\ldots,#n#^c-1\}$ v času $O(c#n#)$.
\end{cor}

\section{Diskusija in Naloge}

Sortiranje je osnovni algoritemski problem v računalništvu in ima dolgo zgodovino.
Knuth \cite{k97v3} pripisuje alogritem sortiranja z zlivanjem
von Neumann(1945). Hitro urejanje je last Hoare \cite{h61}.
Originalno urejanje s kopico je last Williams \cite{w64}, ampak verzija, ki je tu 
predstavljena(v kateri se kopica gradi iz spodaj nazvgor v $O(#n#)$ času) je last Floyd \cite{f64}.
Spodnje meje za sortiranje s primerjavami se zdijo folklorne. Naslednja tabela
povzame izvedbo teh algoritmov za urenjanje s primerjavami:

\begin{center}
  \begin{tabular}{|l|r@{}l@{ }l|l|} \hline
    & \multicolumn{3}{c|}{primerjave} & na mestu  \\ \hline
    Urejanje z zlivanjem & $#n#\log #n#$ & &  najslabši primer & Ne  \\
    Hitrjo urejanje & $1.38#n#\log #n#$ & ${}+ O(#n#)$ & pričakovano & Da \\
    Urejanje s kopico & $2#n#\log #n#$ & ${}+ O(#n#)$ & najslabši primer & Da \\ \hline
  \end{tabular}
\end{center}

Vsi te alogiritmi urejanje s primerjanjem imajo svoje prednosti in slabosti.
Urejanje z zlivanjem naredi najmanj primerjav in se ne zanaša na naključnost.
Na žalost, uporablja pomožno tabelo med fazo zlivanja. Dodeljevanje pomnilnika 
tej tabeli je lahko drago in ima potencial, da je to usodnno za algoritem, če je
količina spomina omejena. Hitro urejanje je algoritem urejanja \emph{na mestu}
\index{in-place algorithm}%
in je blizu na drugem mestu v številu primerjav, ampak je naključno, zato čas 
izvajanja ni vedno zagotovljen. Urejanje s kopico naredi največ primerjav, ampak je 
urejanje na mestu in je deterministično.

Obstaja en primer, v katerem je urejanje s kopico očiten zmagovalec; to se zgodi pri sortiranju
povezanega seznama. V tem primeru, ne potrebujemo pomožne tabele; dva urejena povezana seznama, 
se zelo lahko zljieta v en urejen povezan seznam z uporabo manipulacije kazalcev (glej
\excref{list-merge-sort}).

Algoritma urejanja s štetjem in urejanja po delih opisana tu, je last 
Seward \cite[Section~2.4.6]{s54}. Ampak različice urejanja po delih so
v uporabi že od 20 let 20. stoletja, za urejanje luknjanih kartic
z uporabo strojev za urejanje luknjanih kartic. Te stroji lahko uredijo 
kup kartic v dva kupa, glede na obstoj(ali neobstoj) ljuknice na specifični
lokaciji na kartici. Ponovitev tega procesa, za drugo
luknjico nam da implementacijo urejanja po delih.

Na koncu, opazimo, da urejanje s štetjem in po delih lahko uporabimo, za 
urejanje drugih vrst številk razen ne negativnih celih števil. 
Enostavne spremembe urejanja s štetjem lahko sortirajo cela števila
v kateremkoli intervalu $\{a,\ldots,b\}$, v $O(#n#+b-a)$ času.  Podobno, urejanje 
po delih lahko ureja cela števila na enakem intervalu v $O(#n#(\log_{#n#}(b-a))$ času.
Na koncu, lahko oba algoritma uporabimo za urejanje števil s plavajočo vejico v 
IEEE 754 formatu plavjoče vejice. To lahko naredimo zato, ker je IEEE format narejen
tako, da dovoljuje primerjavo dveh števil s plavajočo vejico glede na njuni vrednosti, 
kot če bi bili celi števili v predznačeni dvojiški predstavitvi \cite{ieee754}.

\begin{exc}
  Ilustriraje izvedbo urejanje z zlivanjem in urejanja s kopico na vhodni tabeli,
  ki vsebuje $1,7,4,6,2,8,3,5$. Naredite vzorčno ilustracijo ene možnosti izvede
  hitrega urejanja na isti tabeli.
\end{exc}

\begin{exc}\exclabel{list-merge-sort}
  Implementirajte verzijo algoritma za urejanje z zlivanjem, ki sortirajo dvojno povezan seznam, brez uporabe
  pomožne tabele. (Glej \excref{dllist-sort}.) 
\end{exc}

\begin{exc}
  Nekatere implementacije #quickSort(a,i,n,c)# vedno uporabljajo #a[i]#
  kot pivot.  V primeru, da je vhodna tabele dolžine #n# v kateri taka implementacija izvede
  $\binom{#n#}{2}$ primerjav.
\end{exc}

\begin{exc}
  Nekatere implementacije #quickSort(a,i,n,c)# vedno uporabljajo #a[i+n/2]#
  kot pivot. V primeru, da je vhodna tabele dolžine #n# v kateri taka 
  implementacija izvede $\binom{#n#}{2}$ primerjav.
\end{exc}

\begin{exc}
  Pokažite, da za katerokoli implementacijo #quickSort(a,i,n,c)#,
  ki izbere pivot deterministično, brez da pogleda katerokoli vrednost 
  v $#a[i]#,\ldots,#a[i+n-1]#$, obstaja vhodna tabela dolžine #n#,
  katera povzroči to implementacijo, da naredi $\binom{#n#}{2}$ primerjav.
\end{exc}

\begin{exc}
  Načrtujte #Comparator#, #c#, katerega lahko podate kot argument funkciji
  #quickSort(a,i,n,c)#, kateri bi povzročil $\binom{#n#}{2}$ primerjav.  
  (Namig: Vašemu Comparator-ju ni potrebno gledati vrednosti, ki se primerjajo.)
\end{exc}

\begin{exc}
  Analizirajte pričakovano število primerjav, ki jih naredi Quicksort, malo bolj pazljivo, kot
  dokaz \thmref{quicksort}. Dokažite, da je pričakovano število primerjav $2#n#H_#n# -#n# + H_#n#$.
\end{exc}

\begin{exc}
  Opišite vhodno tabelo, ki povzroči, da urejanje s kopico, naredi največ
  $2#n#\log #n#-O(#n#)$ primerjav. Utemeljite vaš odgovor.
\end{exc}

\javaonly{
\begin{exc}
  Implementacija sortiranja s kopico, ki je opisana tukaj, uredi elemente v obratni 
  vrstni red in nato  tabelo. Ta zadnji korak, lahko izpustimo, če definiramo nov #Comparator#,
  ki negira rezultat vhoda #Comparatorja# #c#. Razložite zakaj to nebi
  bila dobra optimizacija. (Namig: Pomislite koliko negacij bi bilo potrebno v razmerju s koliko časa potrebujemo, da obrnemo tabelo.)
\end{exc}
}

\begin{exc}
  Najdite nek drug par premutacij $1,2,3$ , ki nisto pravilno urejene
  z drevesom primerjav v \figref{comparison-tree-2}.
\end{exc}

\begin{exc}\exclabel{log-factorial}
  Dokažite, da $\log #n#! = #n#\log #n#-O(#n#)$.
\end{exc}

\begin{exc}
  Dokažite, da dvojiško drevo s $k$ listi ima višino najmanj $\log k$.
\end{exc}

\begin{exc}\exclabel{randomized-lower-bound}
  okažite, da če izberemo naključen list iz dvojiškega drevesa s $k$ listi,
  potem je pričakovana višina lista, najmanj $\log k$.
\end{exc}

\begin{exc}
  Implementacija #radixSort(a,k)# podana tukaj, deluje ko vhodna 
  tabela, #a# vsebuje samo \javaonly{ne negativna} cela števila.
  \javaonly{Razširite to implementacijo, tako da deluje tudi, ko #a# vsebuje negativna in ne negativna cela števila.}
  \cpponly{Napišite verzijo, ki deluje za predznačena cela števila.}
\end{exc}

