\chapter{Implementacija seznama s poljem}
\chaplabel{arrays}
\translatedby{Jan Bratina}{sl}

V tem poglavju si bomo pogledali izvedbe vmesnikov #Seznama# in #Vrste#,
kjer je osnoven podatek hranjen v polju, imenovanem
\emph{podporno polje}.
\index{podporno polje}%
V spodnji tabeli imamo prikazane časovne zahtevnosti operacij za podatkovne strukture predstavljene v tem poglavju:
\newlength{\tabsep}
\setlength{\tabsep}{\itemsep}
\addtolength{\tabsep}{\parsep}
\addtolength{\tabsep}{-2pt}
\begin{center}
\vspace{\tabsep}
\begin{tabular}{|l|l|l|} \hline
& #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# \\ \hline
#ArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\
#ArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#DualArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#RootishArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\ \hline
\end{tabular}
\vspace{\tabsep}
\end{center}
Podatkovne strukture, kjer podatke shranjujemo v enojno polje imajo veliko prednosti,
a tudi omejitev:
\index{arrays}%
\begin{itemize}
\item V polju imamo vedno konstantni čas za dostop do kateregakoli podatka.
To nam omogoča, da se operaciji #get(i)# in #set(i,x)# izvedeta v konstantnem času.

\item Polja niso dinamična. Če želimo vstaviti ali izbrisati element v sredini polja moramo premakniti veliko elementov, da naredimo prostor za novo vstavljen element oz. da zapolnimo praznino po tem, ko smo element izbrisali. Zato je časovna zahtevnost operacij #add(i,x)# in #remove(i)# odvisna od spremenljivk #n# in #i#.

\item Polja ne moremo širiti ali krčiti. Ko imamo večje število elementov, kot je veliko naše podporno polje, moramo ustvariti novo, dovolj veliko polje, v katerega kopiramo podatke iz prejšnjega polja. Ta operacija pa je zelo draga.
\end{itemize}
Tretja točka je zelo pomembna, saj časovne zahtevnosti iz zgornje tabele ne vključujejo spreminjanja velikosti polja. V nadaljevanju bomo videli, da širjenje in krčenje polja ne dodata veliko k \emph{povprečni} časovni zahtevnosti, če jih ustrezno upravljamo. Natančneje, če začnemo s prazno podatkovno strukturo in izvedemo zaporedje operacij $m$ #add(i,x)# ali #remove(i)#
,potem bo časovna zahtevnost širjenja in krčenja polja za $m$ operacij $O(m)$. Čeprav so nekatere operacije dražje je povprečna časovna zahtevnost nad vsemi $m$ operacijami samo $O(1)$ za operacijo.

\cpponly{
V tem poglavju in v celotni knjigi je priročno uporabljati polja, ki imajo števec za velikost. Navadna polja v C++ nimajo te funkcije, zato definiramo razred, #array#, ki hrani dolžino polja. Implementacija tega razreda je enostavna.
Implementiran je kot običajno C++ polje, #a#, in število, #length#:}
\cppimport{ods/array.a.length}
\cpponly{
Velikost polja #array# je določena od kreaciji:
}
\cppimport{ods/array.array(len)}
\cpponly{Elementi v polju so lahko indeksirani:}
\cppimport{ods/array.operator[]}
\cpponly{Na koncu, ko imamo eno polje dodeljeno drugemu, potrebujemo samo še premikanje kazalca, ki pa se izvede v konstantnem času:}
\cppimport{ods/array.operator=}

\section{#ArrayStack#: Implementacija sklada s poljem}
\seclabel{arraystack}

\index{ArrayStack@#ArrayStack#}%
Z operacijo #ArrayStack# implementiramo vmesnik za seznam z uporabo polja #a#, imenovanega the \emph{podporno polje}. Element v seznamu na indeksu #i# je hranjen v #a[i]#. V večini primerov je velikost polja #a# večja, kot je potrebno, zato uporabimo število #n# kot števec števila elementov spravljenih v polju #a#. Tako imamo elemente spravljene v
#a[0]#,\ldots,#a[n-1]# in v vseh primerih velja, $#a.length# \ge #n#$.

\codeimport{ods/ArrayStack.a.n.size()}

\subsection{Osnove}

Dostop in spreminjanje elementov v #ArrayStack# z uporabo operacij #get(i)#
in #set(i,x)# je zelo lahko. Po izvedbi potrebnih mejnih preverjanj polja vrnemo množico oz. #a[i]#.

\codeimport{ods/ArrayStack.get(i).set(i,x)}

Operaciji vstavljanja in brisanja elementov iz #ArrayStack#
sta predstavljeni v \figref{arraystack}. Za implementacijo #add(i,x)#
operacije najprej preverimo če je polje #a# polno. Če je, kličemo metodo #resize()# za povečanje velikosti polja #a#. Kako je metoda #resize()#
implementirana, si bomo pogledali kasneje, saj nas trenutno zanima samo to, da potem, ko kličemo metodo #resize()# še vedno ohranjamo pogoj $#a.length#
> #n#$. Sedaj lahko premaknemo elemente
$#a[i]#,\ldots,#a[n-1]#$ za ena v desno, da naredimo prostor za #x#,
množico #a[i]# spravimo v #x# in povečamo #n#, saj smo vstavili nov element.

\begin{figure}
\begin{center}
\includegraphics[scale=0.90909]{figs/arraystack}
\end{center}
\caption[Dodajanje elementa v ArrayStack]{Zaporedje operacij #add(i,x)# in #remove(i)# v #ArrayStack#. Puščice označujejo elemente, ki jih je potrebno kopirati. Operacije, po katerih moramo klicati metodo #resize()# so označene z zvezdico.}
\figlabel{arraystack}
\end{figure}

\codeimport{ods/ArrayStack.add(i,x)}
Če zapostavimo časovno zahtevnost ob morebitnem klicanju metode #resize()#, potem je časovna zahtevnost operacije #add(i,x)# sorazmerna številu elementov, ki jih moramo premakniti, da naredimo prostor za novo vstavljen element #x#. Zato je časovna zahtevnost operacije (zanemarimo časovno zahtevnost spreminjanja polja #a#) $O(#n#-#i#)$.

Implementacija operacije #remove(i)# je zelo podobna. Premaknemo elemente
$#a[i+1]#,\ldots,#a[n-1]#$ za ena v levo (prepišemo #a[i]#) in zmanjšamo vrednost #n#. Potem preverimo, če števec #n# postaja občutno manjši kot #a.length# s preverjanjem $#a.length# \ge 3#n#$. Če je občutno manjši kličemo metodo #resize()# za zmanjšanje velikosti polja #a#.

\codeimport{ods/ArrayStack.remove(i)}
% TODO: Add shifting figure
Če zanemarimo časovno zahtevnost metode #resize()# je časovna zahtevnost operacije #remove(i)#
sorazmerna s številom elementov, ki jih moramo premakniti. To pomeni, da je časovna zahtevnost $O(#n#-#i#)$.

\translatedby{Luka Zorc}{sl}
\subsection{Večanje in krčenje}

Metoda #resize()# je dokaj enostavna; alocira novo polje #b# velikosti $2#n#$ in skopira #n# elementov iz polja #a# v
prvih #n# mest polja #b# in nato postavi #a# v #b#. Tako po klicu #resize()#, $#a.length# = 2#n#$.

\codeimport{ods/ArrayStack.resize()}

Analiza cene operacije #resize()# je lahka. Metoda naredi polje #b# velikosti $2#n#$ in kopira #n# elementov iz #a#
v #b#. To traja $O(#n#)$ časa.

Pri analizi časa delovanja iz prejšnjega poglavja ni bila všteta cena klica #resize()# funkcije.
V tem poglavju bomo analizirali to ceno z uporabo tehnike znane pod imenom \emph{amortizirana analiza}.
Ta način ne poskuša ugotoviti cene za spreminjanje velikosti med vsako #add(i,x)#
in #remove(i)# operacijo. Namesto tega, se posveti ceni vseh klicev
#resize()# med zaporedjem $m$ klicev funkcije #add(i,x)# ali #remove(i)#.

Predvsem pokažemo:

\begin{lem}\lemlabel{arraystack-amortized}
Če je ustvarjen prazen #ArrayList# in katerokoli zaporedje, ko je $m\ge 1$ kliče #add(i,x)# ali #remove(i)# potem je skupen porabljen čas za vse klice #resize()# enak $O(m)$.
\end{lem}

\begin{proof}
Pokazali bomo, da vsakič ko je klican #resize()#, je število klicev #add# ali #remove# od zadnjega klica #resize()# funkcije, vsaj
$#n#/2-1$. Torej, če $#n#_i$ označuje vrednost #n# med
$i$tim klicem metode #resize()# in $r$ označuje število klicev funkcije
#resize()#, potem je skupno število klicev #add(i,x)# ali
#remove(i)# vsaj
\[
\sum_{i=1}^{r} (#n#_i/2-1) \le m \enspace ,
\]
kar je enako kot
\[
\sum_{i=1}^{r} #n#_i \le 2m + 2r \enspace .
\]
Na drugi strani, je skupno število časa uporabljenega med vsem #resize()# klici enako
\[
\sum_{i=1}^{r} O(#n#_i) \le O(m+r) = O(m) \enspace ,
\]
ker $r$ ni več kot $m$. Vse kar nam ostane je pokazati, da je število klicev #add(i,x)# ali #remove(i)# med $(i-1)$tim
in $i$tim klicem za #resize()# enako vsaj $#n#_i/2$.

Upoštevati moramo dva primera. V prvem primeru, je bila metoda #resize()# klicana
s strani funkcije #add(i,x)#, ker je bilo polje #a# polno, t.j.,
$#a.length# = #n#=#n#_i$. Gledano na prejšnji klic funkcije #resize()#:
je bila velikost #a#-ja po klicu enaka #a.length#, vendar je bilo število
elementov shranjenih v #a#-ju največ $#a.length#/2=#n#_i/2$.
Zdaj pa je število elementov shranjenih v #a# enako $#n#_i=#a.length#$,
torej se je moralo, od prejšnjega klica #resize()# izvesti vsaj $#n#_i/2$ klicev #add(i,x)#.
% TODO: Add figure
Drugi primer se zgodi, ko je #resize()# klicana s strani funkcije
#remove(i)#, ker je $#a.length# \ge 3#n#=3#n#_i$. Enako kot prej
je po prejšnjemu klicu #resize()# bilo število elementov shranjenih
v #a# najmanj $#a.length/2#-1$.\footnote{${}-1$ v tej formuli pomeni
poseben primer ko je $#n#=0$ in $#a.length# = 1$.} Zdaj pa je v #a#
shranjenih $#n#_i\le#a.length#/3$ elementov.
Zato je število #remove(i)# operacij od zadnjega #resize()# klica vsaj
\begin{align*}
R & \ge #a.length#/2 - 1 - #a.length#/3 \\
& = #a.length#/6 - 1 \\
& = (#a.length#/3)/2 - 1 \\
& \ge #n#_i/2 -1\enspace .
\end{align*}
V vsakem primeru je število klicev #add(i,x)# ali #remove(i)#, ki se
zgodijo med $(i-1)$tim klicem za #resize()# in $i$tim klicem za
#resize()# je natanko toliko $#n#_i/2-1$ , kot je tudi potrebno za
dokončanje dokaza.
\end{proof}

\subsection{Povzetek}

Naslednji izrek povzema učinkovitost izvedbe podatkovne strukture #ArrayStack#:

\begin{thm}\thmlabel{arraystack}
#ArrayStack# implementira #List# vmesnik. Z ignoriranjem cene klicev funkcije
#resize()# #ArrayStack# podpira naslednje operacije:
\begin{itemize}
\item #get(i)# in #set(i,x)# v času $O(1)$ a eno operacijo; in
\item #add(i,x)# in #remove(i)# v času $O(1+#n#-#i#)$ na operacijo.
\end{itemize}
Poleg tega, če začnemo z prazno strukturo #ArrayStack# in potem izvajamo katerokoli
zaporedje od $m$ #add(i,x)# in #remove(i)# operacij privede v skupno $O(m)$
časa uporabljenega med vsem klici funkcije #resize()#.
\end{thm}

#ArrayStack# je učinkovit način za implementiranje #Sklada#.
Funkcijo #push(x)# lahko implementiramo kot #add(n,x)# in funkcijo #pop()#
kot #remove(n-1)#, V tem primeru bodo te operacije potrebovale $O(1)$
amortiziranega časa.

\section{#FastArrayStack#: Optimiziran ArrayStack}
\seclabel{fastarraystack}

\index{FastArrayStack@#FastArrayStack#}%
#ArrayStack# opravi večino dela z zamenjevanjem (s
#add(i,x)# in #remove(i)#) in kopiranjem (z #resize()#) podatkov.
V izvedbah prikazanih zgoraj, je bilo to narejeno s pomočjo #for#
zanke. Izkaže se, da ima veliko programskih okolij posebne funkcije,
ki so zelo učinkovite pri kopiranju in premikanju blokov podatkov.
V programskem jeziku C, obstajajo funkcije #memcpy(d,s,n)# in #memmove(d,s,n)#.
V C++ jeziku je #std::copy(a0,a1,b)# algoritem.
V Javi je metoda #System.arraycopy(s,i,d,j,n)#.
\index{memcpy@#memcpy(d,s,n)#}%
\index{std::copy@#std::copy(a0,a1,b)#}%
\index{System.arraycopy@#System.arraycopy(s,i,d,j,n)#}%

\codeimport{ods/FastArrayStack.add(i,x).remove(i).resize()}

Te funkcije so ponavadi zelo optimizirane in lahko uporabljajo tudi posebne strojne ukaze,
ki lahko kopirajo veliko hitreje, kot z uporabo zanke #for#.
Vseeno s pomočjo teh funkcij ne moremo asimptotično zmanjšati izvajalnih časov,
a je ta optimizacija še vedno koristna.
V \lang\ izvedbah Jave, uporaba nativnega
\javaonly{#System.arraycopy(s,i,d,j,n)#}
povzroči pohitritve za faktor med 2 in 3, odvisno od vrste izvajanih operacij.
Izvajane pohitritve se lahko razlikujejo od sistema do sistema.


\translatedby{Jan Tomšič}{sl}
\section{#ArrayQueue#: Vrsta na osnovi polja}
\seclabel{arrayqueue}

\index{ArrayQueue@#ArrayQueue#}%
V tem poglavju bomo predstavili podatkovno strukturo #ArrayQueue#, ki implementira FIFO vrsto; elemente z vrste odstranjujemo (z uporabo operacije #remove()#) v istem vrstnem redu, kot so bili dodani (z uporabo operacije #add(x)#).

Opazimo, da #ArrayStack# ni dobra izbira za izvedbo FIFO vrste in sicer zato, ker moramo izbrati en konec seznama, na katerega dodajamo elemente, nato pa elemente odstranjujemo z drugega konca. Ena izmed operacij mora delovati na glavi seznama, kar vključuje klicanje #add(i,x)# ali #remove(i)#, kjer je vrednost $#i#=0$. To nudi čas izvajanja sorazmeren #n#.

Da bi dosegli učinkovito implementacijo vrste na osnovi seznama, najprej opazimo, da bi bil problem enostaven, če bi imeli neskočno polje #a#. Lahko bi hranili indeks #j#, ki hrani naslednji element za odstranitev ter celo število #n#, ki šteje število elementov v vrsti. Elementi vrste bi bili vedno shranjeni v \[ #a[j]#,#a[j+1]#,\ldots,#a[j+n-1]# \enspace . \]
Sprva bi bila #j# in #n# nastavljena na 0. Na novo dodan element bi uvrstili v #a[j+n]# in povečali #n#. Za odstranitev elementa bi ga odstranili iz #a[j]#, povečali #j# in zmanjšali #n#.

Težava te rešitve je potreba po neskočnem polju. #ArrayQueue# to simulira z uporabo končnega polja in \emph{modularne aritmetike}.
\index{modularna aritmetika}%
To je vrsta aritmetike, ki jo uporabljamo pri napovedovanju časa. Na primer 10:00 plus pet ur je 3:00. Formalno pravimo, da je
\[
10 + 5 = 15 \equiv 3 \pmod{12} \enspace .
\]
Zadnji del enačbe beremo kot ``15 je skladno s 3 po modulu 12.'' Operator $\bmod$ lahko obravnavamo tudi kot binarni operator, da je
\[
15 \bmod 12 = 3 \enspace .
\]

V splošnem je, za celo število $a$ in pozitivno celo število $m$, $a \bmod m$ enolično celo število $r\in\{0,\ldots,m-1\}$ tako, da velja $a = r + km$ za poljubno celo število $k$. Poenostavljeno vrednost $r$ predstavlja ostanek pri deljenju $a$ z $m$. V večini programskih jezikov, vključno \javaonly{z Javo}\cpponly{s C++}, je operator $\bmod$ predstavljen z znakom #%#.\footnote{Temu včasih rečemo operator \emph{brain-dead}, ker nepravilno implementira matematični operator mod, ko je prvi argument negativno število.}

Modularna aritmetika je uporabna za simulacijo neskončnega polje, ker $#i#\bmod #a.length#$ vedno vrne vrednost na intervalu $0,\ldots,#a.length-1#$. Z uporabo modularne aritmetike lahko elemente vrste shranimo na naslednja mesta v polju
\[ #a[j%a.length]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\enspace. \]
To obravnava polje #a# kot \emph{krožno polje}
\index{krožno polje}%
\index{polje!krožno}%
kjer polje indekse večje kot $#a.length#-1$ ``ovije naokrog'' na začetek polja.
% TODO: figure

Paziti moramo le še, da število elementov v #ArrayQueue# ne preseže velikosti #a#.

\codeimport{ods/ArrayQueue.a.j.n}

Zaporedje operacij #add(x)# in #remove()# nad #ArrayQueue# je prikazano na \figref{arrayqueue}. Za izvedbo #add(x)# moramo najprej preveriti, če je #a# poln, in s klicem #resize()# velikost #a# povečati. Nato #x# shranimo v #a[(j+n)%a.length]# in povečamo #n#.

\begin{figure}
\begin{center}
\includegraphics[scale=0.90909]{figs/arrayqueue}
\end{center}
\caption[Dodajanje in odstranjevanje iz ArrayQueue]{Zaporedje operacij #add(x)# in #remove(i)# nad #ArrayQueue#. Puščice označujejo kopiranje elementov. Operacije, ki se zaključijo s klicem #resize()# so označene z zvezdico.}
\figlabel{arrayqueue}
\end{figure}



\codeimport{ods/ArrayQueue.add(x)}

Za izvedbo #remove()# moramo najprej za kasnejšo rabo shraniti #a[j]#. Nato zmanjšamo #n# in povečamo #j# (po modulu #a.length#) tako, da nastavimo $#j#=(#j#+1)\bmod #a.length#$. Na koncu vrnemo shranjeno vrednost #a[j]#. Po potrebi lahko zmanjšamo velikost #a# s klicem #resize()#.

\codeimport{ods/ArrayQueue.remove()}

Operacija #resize()# je zelo podobna operaciji #resize()# pri #ArrayStack#. Dodeli novo polje #b# velikosti $2#n#$ in prepiše
\[
#a[j]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\]
na
\[
#b[0]#,#b[1]#,\ldots,#b[n-1]#
\]
in nastavi $#j#=0$.

\codeimport{ods/ArrayQueue.resize()}

\subsection{Povzetek}

Naslednji izrek povzema učinkovitost podatkovne strukture #ArrayQueue#:

\begin{thm}
#ArrayQueue# implementira vmesnik (FIFO) #Vrste#. Če izvzamemo ceno klica #resize()#, omogoča #ArrayQueue# izvajanje operacij #add(x)# in #remove()# v času $O(1)$ na operacijo. Poleg tega, začenši s prazno vrsto #ArrayQueue#, vsako zaporedje $m$ operacij #add(i,x)# in #remove(i)# porabi skupno $O(m)$ časa skozi vse klice na #resize()#.
\end{thm}

%TODO: Discuss the use of bitwise-and as a replacement for the mod operator

\section{#ArrayDeque#: Hitra obojestranska vrsta z uporabo polja}
\seclabel{arraydeque}

\index{ArrayDeque@#ArrayDeque#}%
Struktura #ArrayQueue# iz prejšnjega poglavja je podatkovna struktura za predstavitev zaporedja, ki omogoča učinkovito dodajanje na en konec in odstranjevanje z drugega konca. Podatkovna struktura #ArrayDeque# pa omogoče tako učinkovito dodajanje kot tudi odstranjevanje z obeh koncev. Ta struktura implementira vmesnik #List# z uporabo enake tehnike krožnega polja, ki je uporabljena pri #ArrayQueue#.

\codeimport{ods/ArrayDeque.a.j.n}

Operaciji #get(i)# in #set(i,x)# nad #ArrayDeque# sta enostavni. Vrneta oziroma nastavita element polja $#a[#{#(j+i)#\bmod#a.length#}#]#$.

\codeimport{ods/ArrayDeque.get(i).set(i,x)}

Implementacija operacije #add(i,x)# je bolj zanimiva. Kot ponavadi, najprej preverimo če je #a# poln in ga po potrebi povečamo s klicem #resize()#. Želimo, da je ta operacija hitra tako, ko je #i# majhen (blizu 0), kot tudi, ko je #i# velik (blizu #n#). Zato preverimo, če drži $#i#<#n#/2$. Če drži, zamaknemo elemente $#a[0]#,\ldots,#a[i-1]#$ za eno mesto v levo. Sicer ($#i#\ge#n#/2$), elemente $#a[i]#,\ldots,#a[n-1]#$ zamaknemo za eno mesto v desno. \figref{arraydeque} prikazuje operaciji #add(i,x)# in #remove(x)# nad #ArrayDeque#.

\begin{figure}
\begin{center}
\includegraphics[scale=0.90909]{figs/arraydeque}
\end{center}
\caption[Dodajanje in odstranjevanje iz ArrayDeque]{Zaporedje operacij #add(i,x)# in #remove(i)# nad #ArrayDeque#. Puščice označujejo prestavljanje elementov.}
\figlabel{arraydeque}
\end{figure}


\codeimport{ods/ArrayDeque.add(i,x)}

S prestavljanjem elementov na tak način zagotovimo, da #add(i,x)# nikoli ne potrebuje prestaviti več not $\min\{ #i#, #n#-#i# \}$ elementov. Čas izvajanja operacije #add(i,x)#, (če ignoriramo ceno operacije #resize()#), je potemtakem $O(1+\min\{#i#,#n#-#i#\})$.

Operacija #remove(i)# je izvedena podobno. Odvisno od $#i#<#n#/2$, #remove(i)# bodisi zamakne elemente $#a[0]#,\ldots,#a[i-1]#$ za eno mesto v desno, bodisi elemente $#a[i+1]#,\ldots,#a[n-1]#$ zamakne za eno mesto v levo. To spet pomeni, da #remove(i)# za zamik elementov nikoli ne potrebuje več kot $O(1+\min\{#i#,#n#-#i#\})$ časa.

\codeimport{ods/ArrayDeque.remove(i)}

\subsection{Povzetek}

Naslednji izrek povzema učinkovitost podatkovne strukture #ArrayDeque#:
\begin{thm}\thmlabel{arraydeque}
#ArrayDeque# implementira vmesnik #List#. Če izvzamemo ceno klica #resize()#, omogoča #ArrayDeque# izvajanje operacij
\begin{itemize}
\item #get(i)# in #set(i,x)# v času $O(1)$ na operacijo; in
\item #add(i,x)# in #remove(i)# v času $O(1+\min\{#i#,#n#-#i#\})$ na operacijo.
\end{itemize}
Poleg tega, začenši s prazno obojestransko vrsto #ArrayDeque#, vsako zaporedje $m$ operacij #add(i,x)# in #remove(i)# porabi skupno $O(m)$ časa skozi vse klice na #resize()#.
\end{thm}

\section{#DualArrayDeque#: Gradnja obojestranske vrste z dveh skladov}
\seclabel{dualarraydeque}

\index{DualArrayDeque@#DualArrayDeque#}%
V sledečem poglavju bomo predstavili podatkovno strukturo #DualArrayDeque#, ki za dosego enakih meja učinkovitosti kot #ArrayDeque#, uporablja dve skladovni polji (#ArrayStack#). Čeprav ni asimptotična učinkovitost #DualArrayDeque# nič boljša kot pri #ArrayDeque#, je struktura vseeno zanimiva, ker nudi dober primer napredne strukture z združitvijo dveh enostavnih.

#DualArrayDeque# predstavlja seznam z uporabo dveh #ArrayStack#ov. Spomnimo se, da #ArrayStack# deluje hitro, ko operacije nad njim spreminjajo elementa z njegovega konca. #DualArrayDeque# sestoji iz dveh #ArrayStack#ov, enega #spredaj# (#front#) in enega #zadaj# (#back#), s konci nasproti, da to operacije hitre na obeh straneh.

\codeimport{ods/DualArrayDeque.front.back}

#DualArrayDeque# ne hrani eksplicitno števila elementov, #n#, ki jih vsebuje. Števila ne rabi hraniti, saj vsebuje $#n#=#front.size()# + #back.size()#$ elementov. Vseeno pa bomo pri analizi #DualArrayDeque# uporabljali #n# za označevanje števila vsebovanih elementov.

\codeimport{ods/DualArrayDeque.size()}

#Sprednji# #ArrayStack# hrani seznam elementov z indeksi $0,\ldots,#front.size()#-1$, vendar jih hrani v obratnem vrstnem vredu. #Zadnji# #ArrayStack# pa hrani seznam elementov z indeksi $#front.size()#,\ldots,#size()#-1$ v normalnem vrstnem redu. Na tak način se #get(i)# in #set(i,x)# prevedeta v primerne klice #get(i)# ali #set(i)# na bodisi #sprednjem# ali #zadnjem# koncu, kar potrebuje $O(1)$ časa na operacijo.

\codeimport{ods/DualArrayDeque.get(i).set(i,x)}

Opazimo da, če je indeks $#i#<#front.size()#$, potem ustreza elementu #spredaj# na položaju $#front.size()#-#i#-1$, ker so elementi #spredaj# shranjeni v obratnem vrstnem redu.

Dodajanje in odstranjevanje elementov iz #DualArrayDeque# je prikazano na sliki \figref{dualarraydeque}. Operacija #add(i,x)# doda element #spredaj# ali #zadaj#, odvisno od situacije:

\begin{figure}
\begin{center}
\includegraphics[scale=0.90909]{figs/dualarraydeque}
\end{center}
\caption[Dodajanje in odstranjevanje v DualArrayDeque]{Zaporedje operacij #add(i,x)# in #remove(i)# nad #DualArrayDeque#. Puščice označujejo prestavljanje elementov. Operacije, po katerih se seznam uravnoteži s klicom #balance()#, so označene z zvezdico.}
\figlabel{dualarraydeque}
\end{figure}



\codeimport{ods/DualArrayDeque.add(i,x)}

Metoda #add(i,x)# uravnoteži #sprednji# in #zadnji# #ArrayStack# s klicom metode #balance()#. Izvedba #balance()# je prikazana spodaj, za enkrat pa je dovolj, če vemo, da razen če je $#size()#<2$, #balance()# poskrbi za to, da se #front.size()# in #back.size()# ne razlikujeta več kot za faktor 3. Natančneje, $3\cdot#front.size()# \ge #back.size()#$ in $3\cdot#back.size()# \ge #front.size()#$.

Nato, analiziramo ceno metode #add(i,x)#, pri tem ne upoštevamo ceno klicev metode #balance()#. Če $#i#<#front.size()#$, potem se #add(i,x)# izvede s klicem na $#front.add(front.size()-i-1,x)#$.  Ker je #front# #ArrayStack# je cena tega
\begin{equation}
  O(#front.size()#-(#front.size()#-#i#-1)+1) = O(#i#+1) \enspace .
  \eqlabel{das-front}
\end{equation}
Po drugi strani pa, če drži $#i#\ge#front.size()#$, potem je #add(i,x)# implementirana kot $#back.add(i-front.size(),x)#$. Cena tega pa je
\begin{equation}
  O(#back.size()#-(#i#-#front.size()#)+1) = O(#n#-#i#+1) \enspace .
  \eqlabel{das-back}
\end{equation}

Opazimo, da se prvi primer \myeqref{das-front} pojavi, ko velja $#i#<#n#/4$.
Drugi primer \myeqref{das-back} se pojavi, ko velja $#i#\ge 3#n#/4$.  Kadar velja $#n#/4\le#i#<3#n#/4$, ne moremo biti prepričani ali delovanje vpliva na #front# ali #back#, ampak v vsakem primeru se postopek izvaja $O(#n#)=O(#i#)=O(#n#-#i#)$ časa, saj je $#i#\ge #n#/4$ in $#n#-#i#>#n#/4$.  Če povzamemo situacijo imamo
\[
     \mbox{Čas izvajanja } #add(i,x)# \le 
          \left\{\begin{array}{ll}
            O(1+ #i#) & \mbox{if $#i#< #n#/4$} \\
            O(#n#) & \mbox{if $#n#/4 \le #i# < 3#n#/4$} \\
            O(1+#n#-#i#) & \mbox{if $#i# \ge 3#n#/4$}
          \end{array}\right.
\]
Tako je čas izvajanja #add(i,x)#, če zanemarimo ceno klicev metode #balance()# sledeč $O(1+\min\{#i#, #n#-#i#\})$.

Metoda #remove(i)# in njene analize spominjajo na #add(i,x)# metodo.

\codeimport{ods/DualArrayDeque.remove(i)}

\subsection{Uravnovešenje}

Osredotočimo se na metodo #balance()# izvedeno z metodo #add(i,x)# in #remove(i)#.  Ta postopek zagotavlja, da niti #front# in niti #back# ne postaneta prevelika (ali premajhna). Zagotavlja, da razen, če obstajata manj kot dva elementa, tako #front# in #back# vsebujeta vsaj $#n#/4$ elementov. Če temu ni tako, potem se premika elemente med njima tako, da #front# in #back# vsebujeta natanko $\lfloor#n#/2\rfloor$ elementov in $\lceil#n#/2\rceil$ elementov.

\codeimport{ods/DualArrayDeque.balance()}

Če metoda #balance()# izvede uravnoteženje, potem premakne $O(#n#)$ elementov in za to potrebuje $O(#n#)$ časa. To je slabo zato, ker je metoda #balance()# klicana z vsakim #add(i,x)# in #remove(i)# klicem.  V vsakem primeruu, sledeč dokaz dokazuje, da metoda #balance()# v povprečju porabi samo konstantno količino časa na operacijo.

\begin{lem}\lemlabel{dualarraydeque-amortized}
Če ustvarimo prazen #DualArrayDeque#, potem zaporedje $m\ge 1$ izvede klice metode #add(i,x)# in #remove(i)#, potem je skupen porabljen čas za klice metode #balance()# $O(m)$.
\end{lem}

\begin{proof}
Dokazali bomo, da če metoda #balance()# premeša elemente, potem je število #add(i,x)# in #remove(i)# operacij vsaj $#n#/2-1$, od kar so bili elementi nazadnje premešani z metodo #balance()#.
Z dokazom v \lemref{arraystack-amortized} lahko dokažemo, da je skupen porabljen čas metode #balance()# $O(m)$.

Izvedli bomo našo analizo z uporabo tehnike, poznane kot \emph{potencialna metoda}.
  \index{potential}%
  \index{potential method}%
  Določimo \emph{potencialni} $\Phi$ za
  #DualArrayDeque# kot razliko v dolžini med #front# in #back#:
  \[  \Phi = |#front.size()# - #back.size()#| \enspace . \]
  Zanimiva stvar glede potenciala je, da klic metode #add(i,x)#
  ali #remove(i)#, ki ne opravi nobenega uravnoteženja, lahko poveča potencial skoraj največ za 1.


  Potrebno je upoštevati, da je takoj po klicu metode #balance()#, ki premeša elemente, potencial $\Phi_0$ največ 1, saj
  \[ \Phi_0 = \left|\lfloor#n#/2\rfloor-\lceil#n#/2\rceil\right|\le 1  \enspace .\]

Razmislite o trenutku takoj pred klicem funkcije #balance()#, ki premeša elemente in domnevajte, da #balance()# premeša elemente zaradi $3#front.size()# < #back.size()#$.
  To opazimo v sledečem primeru,
  \begin{eqnarray*}
   #n# & = & #front.size()#+#back.size()# \\
       & < & #back.size()#/3+#back.size()# \\
       & = & \frac{4}{3}#back.size()#
  \end{eqnarray*}
  Poleg tega je s časom potencial na tem mestu
  \begin{eqnarray*}
  \Phi_1 & = & #back.size()# - #front.size()# \\
      &>& #back.size()# - #back.size()#/3 \\
      &=& \frac{2}{3}#back.size()# \\
      &>& \frac{2}{3}\times\frac{3}{4}#n# \\
      &=& #n#/2
  \end{eqnarray*}
  Zato je število klicev metode #add(i,x)# ali #remove(i)#, od kar je metoda #balance()# nazadnje premešala elemente, najmanj $\Phi_1-\Phi_0 > #n#/2-1$. To zaključuje dokaz.
\end{proof}

\subsection{Povzetek}

Naslednji izrek povzame lastnosti #DualArrayDeque#:

\begin{thm}\thmlabel{dualarraydeque}
  #DualArrayDeque# implementira vmesnik #List#. Z ignoriranjem cene klicev metod #resize()# in #balance()# #DualArrayDeque# podpira operacije
  \begin{itemize}
    \item #get(i)# in #set(i,x)# v času $O(1)$ na operacijo; in
    \item #add(i,x)# in #remove(i)# v času $O(1+\min\{#i#,#n#-#i#\})$ na operacijo.
  \end{itemize}
  Poleg tega, če začnemo z praznim #DualArrayDeque#, potem zaporedje $m$ #add(i,x)# in #remove(i)# metod, konča z skupnim rezultatom $O(m)$ časa porabljenega med vsemi klici metod #resize()# in #balance()#.
\end{thm}

\translatedby{Tadej Mittoni}{sl}
\section{#RootishArrayStack#: A Space-Efficient Array Stack}
\seclabel{rootisharraystack}

\index{RootishArrayStack@#RootishArrayStack#}%
Ena izmed slabosti vseh prejšnjih podatkovnih struktur v tem poglavju je ta, da ker se shranjujejo podatki v eni ali dveh tabelah, ki se izogibajo spreminjanju velikosti, se pogosto zgodi, da so tabele precej prazne.
Na primer, takoj po operaciji #resize()# nad #ArrayStack#-om, je tabela #a# le na pol polna. Še huje, veliko je primerov, kjer samo $1/3$ tabele #a# vsebuje podatke.

Ta razdelek je namenjen podatkovni strukturi #RootishArrayStack#, ki se posveča problemu zapravljenega prostora. #RootishArrayStack# vsebuje #n# elementov z uporabo $O(\sqrt{#n#})$ tabel. V teh tabelah je največ $O(\sqrt{#n#})$ lokacij neuporabljenih v poljubnem času. Vse preostale lokacije v tabeli so uporabljene za shrambo podatkov. Potemtakem te podatkovne strukture zapravijo največ $O(\sqrt{#n#})$ prostora pri shranjevanju #n# elementov.

#RootishArrayStack# shrani svoje elemente v seznam #r# tabel poimenovanih \emph{blocks}, ki so oštevilčene $0,1,\ldots,#r#-1$. Glej \figref{rootisharraystack}. Blok $b$ vsebuje $b+1$ elemente, zato vsi #r# bloki vsebujejo največ 
\[
1+ 2+ 3+\cdots +#r# = #r#(#r#+1)/2
\]
elementov. Zgornja formula se izpelje kot je prikazano na \figref{gauss}.

\begin{figure}
\begin{center}
\includegraphics[width=\ScaleIfNeeded]{figs/rootisharraystack}
\end{center}
\caption[Dodajanje in odstranjevanje v RootishArrayStack]{Sekvenca #add(i,x)# in #remove(i)# operacij na #RootishArrayStack#. Puščice označujejo kopirane elemente.}
\figlabel{rootisharraystack}
\end{figure}

\codeimport{ods/RootishArrayStack.blocks.n}

\begin{figure}
\begin{center}
\includegraphics[scale=0.90909]{figs/gauss}
\end{center}
\caption{Število belih kvadratov je $1+2+3+\cdots+#r#$. Število osenčenih kvadratov je isto. Beli in osenčeni kvadrati skupaj tvorijo pravokotnik, ki vsebuje $#r#(#r#+1)$ kvadratov.}
\figlabel{gauss}
\end{figure}

Kot lahko pričakujemo, so elementi v seznamu razvrščeni po vrsti v bloku. Element v seznamu z indeksom 0 je shranjen v blok 0, elementa z indeksoma 1 in 2 sta shranjena v blok 1, elementi z indeksi 3, 4 in 5 so shranjeni v blok 2, itn. Glavni problem, ki ga je potrebno nasloviti, je pri odločanju, ko nam je podan indeks $#i#$, kateri blok vsebuje tako #i#, kot tudi ustrezni indeks do #i# v samem bloku.

Določanje indeksa #i# v njegovem bloku se izkaže kot lahko. Če je indeks #i# v bloku #b#, potem je število elementov v blokih $0,\ldots,#b#-1$ $#b#(#b#+1)/2$. Potemtakem je #i# shranjen na lokaciji 
\[
#j# = #i# - #b#(#b#+1)/2
\]

v bloku #b#. Malo bolj zahteven je problem določanja vrednosti bloku #b#. Število elementov, ki ima indekse manj ali enake #i# je $#i#+1$. Na drugi strani pa je število elementov v blokih 0,\ldots,b, ki je enako $(#b#+1)(#b#+2)/2$. Potemtakem je #b# najmanjše število, ki še ustreza
\[
(#b#+1)(#b#+2)/2 \ge #i#+1 \enspace .
\]
To enačbo lahko preoblikujemo tako
\[
#b#^2 + 3#b# - 2#i# \ge 0 \enspace .
\]

Ustrezno kvadratna enačba $#b#^2 + 3#b# - 2#i# = 0$ ima dve rešitvi: $#b#=(-3 + \sqrt{9+8#i#}) / 2$ in $#b#=(-3 - \sqrt{9+8#i#}) / 2$.

Druga rešitev nima smisla za našo uporabo, ker da vedno negativno rešitev. Zato uporabimo $#b# = (-3 +
\sqrt{9+8i}) / 2$. V splošnem ta rešitev ni število, vendar če se vrnemo k naši neenakosti, hočemo najmanjšo število $#b#$, tako, da velja $#b# \ge (-3 + \sqrt{9+8i}) / 2$. To je preprosto 
\[
#b# = \left\lceil(-3 + \sqrt{9+8i}) / 2\right\rceil \enspace .
\]

\codeimport{ods/RootishArrayStack.i2b(i)}

Ko je to jasno, sta tudi metodi #get(i)# in #set(i,x)# jasni. Najprej izračunamo ustrezen blok #b# in ustrezen indeks #j# v bloku. Potem izvedemo primerno operacijo:

\codeimport{ods/RootishArrayStack.get(i).set(i,x)}

V primeru, da uporabimo katerokoli podatkovno strukturo v tem poglavju za zastopanje #blocks# seznam, potem se #get(i)# in #set(i,x)# izvajata v konstantnem času.

Metoda #add(i,x)# nam je že poznana. Najprej preverimo, če je naša podatkovna strukura polna tako, da je število blokov #r# tako, da drži $#r#(#r#+1)/2 = #n#$. Če je, pokličemo #grow()#, ki nam doda še en blok. Ko to naredimo, zamaknemo elemente z indeksi $#i#,\ldots,#n#-1$ v desno za eno pozicijo, da naredimo prostor za nov element z indeksom #i#:

\codeimport{ods/RootishArrayStack.add(i,x)}

Metoda #grow()# naredi pričakovano. Doda nov blok:

\codeimport{ods/RootishArrayStack.grow()}

Če ignoriramo ceno operacije #grow()#, potem je cena #add(i,x)# dominirana z vrednostjo zamikanja in je potemtakem enaka $O(1+#n#-#i#)$, kar je enako, kot pri #ArrayStack#.

Operacija #remove(i)# je podobna metodi #add(i,x)#. Le ta zamakne elemente z indeksi $#i#+1,\ldots,#n#$ levo za eno pozicijo. Za tem, če je več kot en blok še prazen, pokliče metodo #shrink()#, da odstrani vse, razen enega še ne uporabljenega bloka:

\codeimport{ods/RootishArrayStack.remove(i)}
\codeimport{ods/RootishArrayStack.shrink()}

Če spet ignoriramo ceno operacije #shrink()#, je cena #remove(i)# dominirana z vrednostjo zamikanja in je potemtakem enaka $O(#n#-#i#)$.

\subsection{Analiza rasti in krčenja}

Zgornja analiza #add(i,x)# in #remove(i)# ne vzema v zakup cene metodi #grow()# in #shrink()#. Upoštevajte, da metodi #grow()# in #shrink()# ne kopirata nobenih podatkov, kot to dela operacija #ArrayStack.resize()#, temveč le alocirajo ali izpraznijo tabelo velikosti #r#. V določenih okoljih se to zgodi v konstantnem času, dočim zna v drugih to zahtevati proporcionalen čas glede na #r#.

Takoj po klicu #grow()# ali #shrink()# se situacija počisti. Zanji blok je popolnoma prazen, vsi ostali pa so povsem zapolnjeni. Dodaten klic #grow()# ali #shrink()# se ne bo zgodil dokler vsaj $#r#-1$ elementov ni bilo dodanih ali odstranjenih. Četudi vzamejo #grow()# in #shrink()# $O(#r#)$ časa, je lahko vrednost cene #grow()# in #shrink()# amortizirana na $O(1)$ za vsako posamezno operacijo.

\subsection{Poraba prostora}
\seclabel{rootishspaceusage}

Sedaj bomo analizirali količino dodatnega prostora, ki ga uporablja #RootishArrayStack#. Bolj natančno, hočemo prešteti ves prostor, ki ga uporablja #RootishArrayStack# in le ta ni element tabele, ki je trenutno uporabljen za držanje elementa seznama. Takemu prostoru rečemo \emph{wasted space}.
\index{wasted space}%

Operacija #remove(i)# zagotavlja, da #RootishArrayStack# nikoli nima več kot dva zapolnjena bloka. Število blokov, #r#, uporabljenih s strani #RootishArrayStack#, ki imajo shranjenih #n# elementov potemtakem zadovoljijo
\[
(#r#-2)(#r#-1) \le #n# \enspace .
\]
Če uporabimo kvadratno enačbo nam da
\[
#r# \le (3+\sqrt{1+4#n#})/2 = O(\sqrt{#n#}) \enspace .
\]

Zadnje dva bloka sta velikosti #r# in #r-1#, zato je največ zapravljenega prostora $2#r#-1 = O(\sqrt{#n#})$. Če shranimo bloka v (npr.) #ArrayList#, ima potem #List#, ki shranjuje #r# bloke, $O(#r#)=O(\sqrt{#n#})$ zapravljenega prostora. Ostali prostor, ki ga potrebujemo za shrambo #n# in ostalih informacij je potemtakem $O(1)$. Skupaj je zapravljenega prostora v #RootishArrayStack# $O(\sqrt{#n#})$.

Nato trdimo, da je tak način uporabe prostora optimalen za katerokoli podatkovno strukturo, ki je na začetku prazna in podpira seštevanje enega elementa v določenem času. Bolj natančno smo zmožni prikazati, da v točno določenem času med seštevanjem #n# elementov, podatkovna struktura zapravlja vsaj $\sqrt{#n#}$ prostora (čeprav je to le za trenutek).

Predpostavimo, da začnemo s prazno podatkovno strukturo in dodamo #n# elementov vsakega posebej. Na koncu procesa je vseh #n# elementov shranjenih v strukturi in porazdeljenih med #r# kolekcijo spominskih blokov. Če velja $#r#\ge \sqrt{#n#}$, potem mora podatkovna struktura uporabljati #r# kazalcev (ali referenc), da sledi vsem #r# blokom. Te kazalci so zapravljen prostor. Na drugi strani, če velja $#r# < \sqrt{#n#}$, potem morajo zaradi načela predalčkanja, določeni bloki biti vsaj $#n#/#r# > \sqrt{#n#}$ veliki. Vpoštevajoč moment v katerem je bil blok najprej alociran. Takoj po alociranju, je bil blok prazen in je zato zapravljal $\sqrt{#n#}$ prostora. Zaradi tega je bilo ob točno določenem času med vstavljanjem #n# elemntov, zapravljenega  $\sqrt{#n#}$ prostora s strani podatkovne strukture. 

\subsection{Povzetek}

Sledeč teorem povzema našo diskusijo o podatkovni strukturi #RootishArrayStack#:

\begin{thm}\thmlabel{rootisharraystack}
#RootishArrayStack# implementira vmesnik #List#. #RootishArrayStack# ignorira cene klicev metod #grow()# in #shrink()# ter podpira operacije
\begin{itemize}
\item #get(i)# in #set(i,x)# z $O(1)$ časom na operacijo; in
\item #add(i,x)# in #remove(i)# z $O(1+#n#-#i#)$ časom na operacijo.
\end{itemize}

Še več, če začnemo s praznim #RootishArrayStack#, bo katerakoli sekvenca $m$ #add(i,x)# in #remove(i)# operacij potrebovala v celoti $O(m)$ časa za vse klice teh dveh metod.

Prostor (merjen v besedah),\footnote{Spomnimo se \secref{model} za diskusijo kako se meri spomin.} ki ga #RootishArrayStack# porabi za shrambo #n# elementov, je $#n# +O(\sqrt{#n#})$.
\end{thm}

\subsection{Computing Square Roots}

\index{square roots}%
A reader who has had some exposure to models of computation may notice
that the #RootishArrayStack#, as described above, does not fit into
the usual word-RAM model of computation (\secref{model}) because it
requires taking square roots. The square root operation is generally
not considered a basic operation and is therefore not usually part of
the word-RAM model.

In this section, we show that the square root operation can be
implemented efficiently. In particular, we show that for any integer
$#x#\in\{0,\ldots,#n#\}$, $\lfloor\sqrt{#x#}\rfloor$ can be computed
in constant-time, after $O(\sqrt{#n#})$ preprocessing that creates two
arrays of length $O(\sqrt{#n#})$. The following lemma shows that we
can reduce the problem of computing the square root of #x# to the square
root of a related value #x'#.

\begin{lem}\lemlabel{root}
Let $#x#\ge 1$ and let $#x'#=#x#-a$, where $0\le a\le\sqrt{#x#}$. Then
$\sqrt{x'} \ge \sqrt{#x#}-1$.
\end{lem}

\begin{proof}
It suffices to show that
\[
\sqrt{#x#-\sqrt{#x#}} \ge \sqrt{#x#}-1 \enspace .
\]
Square both sides of this inequality to get
\[
#x#-\sqrt{#x#} \ge #x#-2\sqrt{#x#}+1
\]
and gather terms to get
\[
\sqrt{#x#} \ge 1
\]
which is clearly true for any $#x#\ge 1$.
\end{proof}

Start by restricting the problem a little, and assume that $2^{#r#} \le
#x# < 2^{#r#+1}$, so that $\lfloor\log #x#\rfloor=#r#$, i.e., #x# is an
integer having $#r#+1$ bits in its binary representation. We can take
$#x'#=#x# - (#x#\bmod 2^{\lfloor r/2\rfloor})$. Now, #x'# satisfies
the conditions of \lemref{root}, so $\sqrt{#x#}-\sqrt{#x'#} \le 1$.
Furthermore, #x'# has all of its lower-order $\lfloor #r#/2\rfloor$ bits
equal to 0, so there are only
\[
2^{#r#+1-\lfloor #r#/2\rfloor} \le 4\cdot2^{#r#/2} \le 4\sqrt{#x#}
\]
possible values of #x'#. This means that we can use an array, #sqrttab#,
that stores the value of $\lfloor\sqrt{#x'#}\rfloor$ for each possible
value of #x'#. A little more precisely, we have
\[
#sqrttab#[i]
= \left\lfloor
\sqrt{i 2^{\lfloor #r#/2\rfloor}}
\right\rfloor \enspace .
\]
In this way, $#sqrttab#[i]$ is within 2 of $\sqrt{#x#}$ for all
$#x#\in\{i2^{\lfloor r/2\rfloor},\ldots,(i+1)2^{\lfloor r/2\rfloor}-1\}$.
Stated another way, the array entry
$#s#=#sqrttab#[#x##>>#\lfloor #r#/2\rfloor]$ is either equal to
$\lfloor\sqrt{#x#}\rfloor$,
$\lfloor\sqrt{#x#}\rfloor-1$, or
$\lfloor\sqrt{#x#}\rfloor-2$. From #s# we can determine the value
of $\lfloor\sqrt{#x#}\rfloor$ by
incrementing #s# until
$(#s#+1)^2 > #x#$.
\codeimport{ods/FastSqrt.sqrt(x,r)}

Now, this only works for $#x#\in\{2^{#r#},\ldots,2^{#r#+1}-1\}$ and
#sqrttab# is a special table that only works for a particular value
of $#r#=\lfloor\log #x#\rfloor$. To overcome this, we could compute
$\lfloor\log #n#\rfloor$ different #sqrttab# arrays, one for each possible
value of $\lfloor\log #x#\rfloor$. The sizes of these tables form an exponential sequence whose largest value is at most $4\sqrt{#n#}$, so the total size of all tables is $O(\sqrt{#n#})$.

However, it turns out that more than one #sqrttab# array is unnecessary;
we only need one #sqrttab# array for the value $#r#=\lfloor\log
#n#\rfloor$. Any value #x# with $\log#x#=#r'#<#r#$ can be \emph{upgraded}
by multiplying #x# by $2^{#r#-#r'#}$ and using the equation
\[
\sqrt{2^{#r#-#r'#}x} = 2^{(#r#-#r#')/2}\sqrt{#x#} \enspace .
\]
The quantity $2^{#r#-#r#'}x$ is in the range
$\{2^{#r#},\ldots,2^{#r#+1}-1\}$ so we can look up its square root
in #sqrttab#. The following code implements this idea to compute
$\lfloor\sqrt{#x#}\rfloor$ for all non-negative integers #x# in the
range $\{0,\ldots,2^{30}-1\}$ using an array, #sqrttab#, of size $2^{16}$.
\codeimport{ods/FastSqrt.sqrt(x)}

Something we have taken for granted thus far is the question of how
to compute
$#r#'=\lfloor\log#x#\rfloor$. Again, this is a problem that can be solved
with an array, #logtab#, of size $2^{#r#/2}$. In this case, the
code is particularly simple, since $\lfloor\log #x#\rfloor$ is just the
index of the most significant 1 bit in the binary representation of #x#.
This means that, for $#x#>2^{#r#/2}$, we can right-shift the bits of
#x# by $#r#/2$ positions before using it as an index into #logtab#.
The following code does this using an array #logtab# of size $2^{16}$ to compute
$\lfloor\log #x#\rfloor$ for all #x# in the range $\{1,\ldots,2^{32}-1\}$.
\codeimport{ods/FastSqrt.log(x)}

Finally, for completeness, we include the following code that initializes #logtab# and #sqrttab#:
\codeimport{ods/FastSqrt.inittabs()}

To summarize, the computations done by the #i2b(i)# method can be
implemented in constant time on the word-RAM using $O(\sqrt{n})$ extra
memory to store the #sqrttab# and #logtab# arrays. These arrays can be
rebuilt when #n# increases or decreases by a factor of two, and the cost
of this rebuilding can be amortized over the number of #add(i,x)# and
#remove(i)# operations that caused the change in #n# in the same way that
the cost of #resize()# is analyzed in the #ArrayStack# implementation.


\section{Discussion and Exercises}

Most of the data structures described in this chapter are folklore. They
can be found in implementations dating back over 30 years. For example,
implementations of stacks, queues, and deques, which generalize easily
to the #ArrayStack#, #ArrayQueue# and #ArrayDeque# structures described
here, are discussed by Knuth \cite[Section~2.2.2]{k97v1}.

Brodnik \etal\ \cite{bcdms99} seem to have been the first to describe
the #RootishArrayStack# and prove a $\sqrt{n}$ lower-bound like that
in \secref{rootishspaceusage}. They also present a different structure
that uses a more sophisticated choice of block sizes in order to avoid
computing square roots in the #i2b(i)# method. Within their scheme,
the block containing #i# is block $\lfloor\log (#i#+1)\rfloor$, which
is simply the index of the leading 1 bit in the binary representation
of $#i#+1$. Some computer architectures provide an instruction for
computing the index of the leading 1-bit in an integer. \javaonly{In
Java, the #Integer# class provides a method #numberOfLeadingZeros(i)#
from which one can easily compute $\lfloor\log (#i#+1)\rfloor$.}

A structure related to the #RootishArrayStack# is the two-level
\emph{tiered-vector} of Goodrich and Kloss \cite{gk99}.
\index{tiered-vector}%
This structure
supports the #get(i,x)# and #set(i,x)# operations in constant time and
#add(i,x)# and #remove(i)# in $O(\sqrt{#n#})$ time. These running times
are similar to what can be achieved with the more careful implementation
of a #RootishArrayStack# discussed in \excref{rootisharraystack-fast}.

\javaonly{
\begin{exc}
In the #ArrayStack# implementation, after the first call to #remove(i)#,
the backing array, #a#, contains $#n#+1$ non-#null# values despite
the fact that the #ArrayStack# only contains #n# elements. Where is
the extra non-#null# value? Discuss any consequences this non-#null#
value might have on the Java Runtime Environment's memory manager.
\index{Java Runtime Environment}%
\index{memory manager}%
\end{exc}
}

\begin{exc}
The #List# method #addAll(i,c)# inserts all elements of the #Collection#
#c# into the list at position #i#. (The #add(i,x)# method is a special
case where $#c#=\{#x#\}$.) Explain why, for the data structures
in this chapter, it is not efficient to implement #addAll(i,c)# by
repeated calls to #add(i,x)#. Design and implement a more efficient
implementation.
\end{exc}

\begin{exc}
Design and implement a \emph{#RandomQueue#}.
\index{RandomQueue@#RandomQueue#}%
This is an implementation
of the #Queue# interface in which the #remove()# operation removes
an element that is chosen uniformly at random among all the elements
currently in the queue. (Think of a #RandomQueue# as a bag in which
we can add elements or reach in and blindly remove some random element.)
The #add(x)# and #remove()# operations in a #RandomQueue# should run
in constant time per operation.
\end{exc}

\begin{exc}
Design and implement a #Treque# (triple-ended queue).
\index{Treque@#Treque#}%
This is a #List#
implementation in which #get(i)# and #set(i,x)# run in constant time
and #add(i,x)# and #remove(i)# run in time
\[
O(1+\min\{#i#, #n#-#i#, |#n#/2-#i#|\}) \enspace .
\]
In other words, modifications are fast if they are near either
end or near the middle of the list.
\end{exc}

\begin{exc}
Implement a method #rotate(a,r)# that ``rotates'' the array #a#
so that #a[i]# moves to $#a#[(#i#+#r#)\bmod #a.length#]$, for all
$#i#\in\{0,\ldots,#a.length#\}$.
\end{exc}

\begin{exc}
Implement a method #rotate(r)# that ``rotates'' a #List# so that
list item #i# becomes list item $(#i#+#r#)\bmod #n#$. When run on
an #ArrayDeque#, or a #DualArrayDeque#, #rotate(r)# should run in
$O(1+\min\{#r#,#n#-#r#\})$ time.
\end{exc}

\begin{exc}
Modify the #ArrayDeque# implementation so that the shifting
done by #add(i,x)#, #remove(i)#, and #resize()# is done using
the faster #System.arraycopy(s,i,d,j,n)# method.
\end{exc}

\begin{exc}
Modify the #ArrayDeque# implementation so that it does not use the
#%# operator (which is expensive on some systems). Instead, it
should make use of the fact that, if #a.length# is a power of 2,
then
\[ #k%a.length#=#k&(a.length-1)# \enspace .
\]
(Here, #&# is the bitwise-and operator.)
\end{exc}

\begin{exc}
Design and implement a variant of #ArrayDeque# that does not do any
modular arithmetic at all. Instead, all the data sits in a consecutive
block, in order, inside an array. When the data overruns the beginning
or the end of this array, a modified #rebuild()# operation is performed.
The amortized cost of all operations should be the same as in an
#ArrayDeque#.

\noindent Hint: Getting this to work is really all about how you implement
the #rebuild()# operation. You would like #rebuild()# to put the data
structure into a state where the data cannot run off either end until
at least $#n#/2$ operations have been performed.

Test the performance of your implementation against the #ArrayDeque#.
Optimize your implementation (by using #System.arraycopy(a,i,b,i,n)#)
and see if you can get it to outperform the #ArrayDeque# implementation.
\end{exc}

\begin{exc}
Design and implement a version of a #RootishArrayStack# that has
only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)#
and #remove(i,x)# operations in $O(1+\min\{#i#,#n#-#i#\})$ time.
\end{exc}

\begin{exc}\exclabel{rootisharraystack-fast}
Design and implement a version of a #RootishArrayStack# that has
only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)#
and #remove(i,x)# operations in $O(1+\min\{\sqrt{#n#},#n#-#i#\})$
time. (For an idea on how to do this, see \secref{selist}.)
\end{exc}

\begin{exc}
Design and implement a version of a #RootishArrayStack# that has
only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)# and
#remove(i,x)# operations in $O(1+\min\{#i#,\sqrt {#n#},#n#-#i#\})$ time.
(See \secref{selist} for ideas on how to achieve this.)
\end{exc}

\begin{exc}
Design and implement a #CubishArrayStack#.
\index{CubishArrayStack@#CubishArrayStack#}%
This three level structure
implements the #List# interface using $O(n^{2/3})$ wasted space.
In this structure, #get(i)# and #set(i,x)# take constant time; while
#add(i,x)# and #remove(i)# take $O(#n#^{1/3})$ amortized time.
\end{exc}
